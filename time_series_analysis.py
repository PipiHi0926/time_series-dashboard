import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, List, Tuple, Optional
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

def time_series_analysis_page():
    """æ™‚åºåˆ†æé é¢"""
    st.header("ğŸ“ˆ æ™‚åºåˆ†æ")
    
    if st.session_state.fab_data is None:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³æ•¸æ“šä¸¦é¸æ“‡ FAB")
        st.info("ğŸ’¡ è«‹å…ˆå¾å·¦å´é¸å–®é¸æ“‡ã€ŒKPI å¿«é€Ÿåˆ†æã€è¼‰å…¥æ•¸æ“š")
        return
    
    fab_data = st.session_state.fab_data
    selected_fab = st.session_state.selected_fab
    available_kpis = st.session_state.available_kpis
    
    # é¡¯ç¤ºç•¶å‰é¸æ“‡
    st.info(f"ğŸ­ ç•¶å‰ FAB: **{selected_fab}** | ğŸ“Š ç•¶å‰ KPI: **{st.session_state.selected_kpi}**")
    
    st.subheader(f"ğŸ­ {selected_fab} - æ™‚åºåˆ†æ")
    
    # é¸æ“‡åˆ†ææ–¹æ³•
    analysis_type = st.selectbox(
        "é¸æ“‡åˆ†ææ–¹æ³•:",
        ["è¶¨å‹¢åˆ†æ", "é€±æœŸæ€§åˆ†æ", "è‡ªç›¸é—œåˆ†æ", "è®Šé»æª¢æ¸¬", "æ™‚åºåˆ†è§£", "ç•°å¸¸æ¨¡å¼åˆ†æ"]
    )
    
    # é¸æ“‡ KPI
    if analysis_type in ["è¶¨å‹¢åˆ†æ", "é€±æœŸæ€§åˆ†æ", "è‡ªç›¸é—œåˆ†æ", "è®Šé»æª¢æ¸¬", "æ™‚åºåˆ†è§£"]:
        selected_kpi = st.selectbox(
            "é¸æ“‡è¦åˆ†æçš„ KPI:",
            options=available_kpis
        )
        
        if st.button("ğŸ” åŸ·è¡Œåˆ†æ"):
            kpi_data = fab_data[fab_data['KPI'] == selected_kpi].copy()
            kpi_data = kpi_data.sort_values('REPORT_TIME')
            
            if len(kpi_data) == 0:
                st.error("âŒ æ‰€é¸ KPI ç„¡è³‡æ–™")
                return
            
            if analysis_type == "è¶¨å‹¢åˆ†æ":
                trend_analysis(kpi_data, selected_kpi, selected_fab)
            elif analysis_type == "é€±æœŸæ€§åˆ†æ":
                periodicity_analysis(kpi_data, selected_kpi, selected_fab)
            elif analysis_type == "è‡ªç›¸é—œåˆ†æ":
                autocorrelation_analysis(kpi_data, selected_kpi, selected_fab)
            elif analysis_type == "è®Šé»æª¢æ¸¬":
                changepoint_detection(kpi_data, selected_kpi, selected_fab)
            elif analysis_type == "æ™‚åºåˆ†è§£":
                time_series_decomposition(kpi_data, selected_kpi, selected_fab)
    
    elif analysis_type == "ç•°å¸¸æ¨¡å¼åˆ†æ":
        selected_kpis = st.multiselect(
            "é¸æ“‡è¦åˆ†æçš„ KPI (æœ€å¤š8å€‹):",
            options=available_kpis,
            default=available_kpis[:min(4, len(available_kpis))],
                    )
        
        if st.button("ğŸ” åŸ·è¡Œåˆ†æ"):
            if len(selected_kpis) < 2:
                st.warning("âš ï¸ ç•°å¸¸æ¨¡å¼åˆ†æè‡³å°‘éœ€è¦é¸æ“‡2å€‹KPI")
                return
            
            anomaly_pattern_analysis(fab_data, selected_kpis, selected_fab)

def trend_analysis(kpi_data: pd.DataFrame, kpi_name: str, fab_name: str):
    """è¶¨å‹¢åˆ†æ"""
    st.subheader("ğŸ“Š è¶¨å‹¢åˆ†æçµæœ")
    
    values = kpi_data['VALUE'].values
    dates = kpi_data['REPORT_TIME'].values
    
    # ç·šæ€§è¶¨å‹¢
    x = np.arange(len(values))
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, values)
    trend_line = slope * x + intercept
    
    # è¶¨å‹¢çµ±è¨ˆ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        trend_direction = "ä¸Šå‡" if slope > 0 else "ä¸‹é™" if slope < 0 else "ç©©å®š"
        st.metric("è¶¨å‹¢æ–¹å‘", trend_direction)
    
    with col2:
        st.metric("æ–œç‡", f"{slope:.4f}")
    
    with col3:
        st.metric("ç›¸é—œä¿‚æ•¸", f"{r_value:.3f}")
    
    with col4:
        significance = "é¡¯è‘—" if p_value < 0.05 else "ä¸é¡¯è‘—"
        st.metric("è¶¨å‹¢é¡¯è‘—æ€§", significance)
    
    # è¶¨å‹¢åœ–
    fig = go.Figure()
    
    # åŸå§‹æ•¸æ“š
    fig.add_trace(go.Scatter(
        x=dates,
        y=values,
        mode='lines+markers',
        name='åŸå§‹æ•¸æ“š',
        line=dict(color='blue', width=2),
        marker=dict(size=4)
    ))
    
    # è¶¨å‹¢ç·š
    fig.add_trace(go.Scatter(
        x=dates,
        y=trend_line,
        mode='lines',
        name=f'ç·šæ€§è¶¨å‹¢ (æ–œç‡={slope:.4f})',
        line=dict(color='red', width=2, dash='dash')
    ))
    
    # ç§»å‹•å¹³å‡
    window_sizes = [7, 30, 90]
    colors = ['green', 'orange', 'purple']
    
    for window, color in zip(window_sizes, colors):
        if len(values) >= window:
            ma = pd.Series(values).rolling(window=window).mean()
            fig.add_trace(go.Scatter(
                x=dates,
                y=ma,
                mode='lines',
                name=f'{window}æ—¥ç§»å‹•å¹³å‡',
                line=dict(color=color, width=1.5)
            ))
    
    fig.update_layout(
        title=f"{fab_name} - {kpi_name} è¶¨å‹¢åˆ†æ",
        xaxis_title="æ™‚é–“",
        yaxis_title="æ•¸å€¼",
        hovermode='x unified',
        height=500
    )
    
    st.plotly_chart(fig)
    
    # è¶¨å‹¢è®ŠåŒ–ç‡åˆ†æ
    st.subheader("ğŸ“ˆ è¶¨å‹¢è®ŠåŒ–ç‡åˆ†æ")
    
    # è¨ˆç®—ä¸åŒæ™‚é–“çª—å£çš„è®ŠåŒ–ç‡
    periods = [7, 30, 90, 180]
    change_rates = []
    
    for period in periods:
        if len(values) >= period:
            recent_avg = np.mean(values[-period:])
            previous_avg = np.mean(values[-2*period:-period]) if len(values) >= 2*period else np.mean(values[:-period])
            change_rate = ((recent_avg - previous_avg) / previous_avg) * 100
            change_rates.append(change_rate)
        else:
            change_rates.append(None)
    
    change_df = pd.DataFrame({
        'æ™‚é–“çª—å£': [f'è¿‘{p}å¤©' for p in periods],
        'è®ŠåŒ–ç‡(%)': [f"{rate:.2f}%" if rate is not None else "N/A" for rate in change_rates]
    })
    
    st.dataframe(change_df)

def periodicity_analysis(kpi_data: pd.DataFrame, kpi_name: str, fab_name: str):
    """é€±æœŸæ€§åˆ†æ"""
    st.subheader("ğŸ”„ é€±æœŸæ€§åˆ†æçµæœ")
    
    values = kpi_data['VALUE'].values
    dates = kpi_data['REPORT_TIME'].values
    
    if len(values) < 14:
        st.warning("âš ï¸ æ•¸æ“šé»å¤ªå°‘ï¼Œç„¡æ³•é€²è¡Œé€±æœŸæ€§åˆ†æ")
        return
    
    # FFT åˆ†æ
    fft = np.fft.fft(values)
    freqs = np.fft.fftfreq(len(values))
    
    # æ‰¾å‡ºä¸»è¦é€±æœŸ
    power = np.abs(fft) ** 2
    main_freq_idx = np.argsort(power[1:len(power)//2])[-5:] + 1  # å‰5å€‹ä¸»è¦é »ç‡
    main_periods = [1/freqs[idx] for idx in main_freq_idx if freqs[idx] > 0]
    
    # é€±æœŸæ€§æŒ‡æ¨™
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**æª¢æ¸¬åˆ°çš„ä¸»è¦é€±æœŸ:**")
        for i, period in enumerate(main_periods[:3]):
            if period <= len(values):
                st.write(f"{i+1}. {period:.1f} å¤©")
    
    with col2:
        # é€±é–“æ¨¡å¼åˆ†æ
        kpi_data['day_of_week'] = pd.to_datetime(kpi_data['REPORT_TIME']).dt.day_name()
        weekly_stats = kpi_data.groupby('day_of_week')['VALUE'].agg(['mean', 'std']).round(2)
        st.write("**é€±é–“æ¨¡å¼çµ±è¨ˆ:**")
        st.dataframe(weekly_stats)
    
    # è¦–è¦ºåŒ–
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('åŸå§‹æ™‚åº', 'é »è­œåˆ†æ', 'é€±é–“æ¨¡å¼', 'è‡ªç›¸é—œå‡½æ•¸'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # åŸå§‹æ™‚åº
    fig.add_trace(
        go.Scatter(x=dates, y=values, mode='lines', name='åŸå§‹æ•¸æ“š'),
        row=1, col=1
    )
    
    # é »è­œ
    fig.add_trace(
        go.Scatter(x=freqs[1:len(freqs)//2], y=power[1:len(power)//2], 
                   mode='lines', name='åŠŸç‡è­œ'),
        row=1, col=2
    )
    
    # é€±é–“æ¨¡å¼
    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    weekly_means = [weekly_stats.loc[day, 'mean'] if day in weekly_stats.index else 0 for day in day_order]
    
    fig.add_trace(
        go.Bar(x=day_order, y=weekly_means, name='é€±é–“å¹³å‡'),
        row=2, col=1
    )
    
    # è‡ªç›¸é—œ
    autocorr = [np.corrcoef(values[:-i], values[i:])[0,1] for i in range(1, min(50, len(values)//2))]
    fig.add_trace(
        go.Scatter(x=list(range(1, len(autocorr)+1)), y=autocorr, 
                   mode='lines+markers', name='è‡ªç›¸é—œ'),
        row=2, col=2
    )
    
    fig.update_layout(height=600, showlegend=False)
    st.plotly_chart(fig)

def autocorrelation_analysis(kpi_data: pd.DataFrame, kpi_name: str, fab_name: str):
    """è‡ªç›¸é—œåˆ†æ"""
    st.subheader("ğŸ”— è‡ªç›¸é—œåˆ†æçµæœ")
    
    values = kpi_data['VALUE'].values
    
    if len(values) < 20:
        st.warning("âš ï¸ æ•¸æ“šé»å¤ªå°‘ï¼Œç„¡æ³•é€²è¡Œè‡ªç›¸é—œåˆ†æ")
        return
    
    # è¨ˆç®—è‡ªç›¸é—œå’Œåè‡ªç›¸é—œ
    max_lags = min(40, len(values) // 4)
    lags = range(1, max_lags + 1)
    
    # è‡ªç›¸é—œ
    autocorr = [np.corrcoef(values[:-lag], values[lag:])[0,1] for lag in lags]
    
    # æ‰¾å‡ºé¡¯è‘—çš„è‡ªç›¸é—œ
    significant_lags = [lag for lag, corr in zip(lags, autocorr) if abs(corr) > 0.2]
    
    # çµ±è¨ˆä¿¡æ¯
    col1, col2, col3 = st.columns(3)
    
    with col1:
        max_autocorr = max(autocorr)
        max_lag = lags[autocorr.index(max_autocorr)]
        st.metric("æœ€å¤§è‡ªç›¸é—œ", f"{max_autocorr:.3f}", f"å»¶é²{max_lag}å¤©")
    
    with col2:
        st.metric("é¡¯è‘—è‡ªç›¸é—œæ•¸é‡", len(significant_lags))
    
    with col3:
        persistence = sum(1 for corr in autocorr[:7] if corr > 0.1)  # å‰7å¤©çš„æŒçºŒæ€§
        st.metric("çŸ­æœŸæŒçºŒæ€§", f"{persistence}/7å¤©")
    
    # è¦–è¦ºåŒ–
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=('è‡ªç›¸é—œå‡½æ•¸', 'é¡¯è‘—æ€§æª¢é©—'),
        vertical_spacing=0.1
    )
    
    # è‡ªç›¸é—œå‡½æ•¸
    fig.add_trace(
        go.Scatter(x=list(lags), y=autocorr, 
                   mode='lines+markers', name='è‡ªç›¸é—œ'),
        row=1, col=1
    )
    
    # æ·»åŠ ç½®ä¿¡å€é–“
    confidence_level = 1.96 / np.sqrt(len(values))
    fig.add_hline(y=confidence_level, line_dash="dash", line_color="red", row=1, col=1)
    fig.add_hline(y=-confidence_level, line_dash="dash", line_color="red", row=1, col=1)
    
    # é¡¯è‘—æ€§æ¢å½¢åœ–
    colors = ['red' if abs(corr) > confidence_level else 'blue' for corr in autocorr]
    fig.add_trace(
        go.Bar(x=list(lags), y=autocorr, marker_color=colors, name='é¡¯è‘—æ€§'),
        row=2, col=1
    )
    
    fig.update_layout(height=600, showlegend=False)
    st.plotly_chart(fig)
    
    # é¡¯è‘—æ»¯å¾Œåˆ†æ
    if significant_lags:
        st.subheader("ğŸ“‹ é¡¯è‘—è‡ªç›¸é—œæ»¯å¾Œ")
        sig_df = pd.DataFrame({
            'æ»¯å¾Œå¤©æ•¸': significant_lags,
            'è‡ªç›¸é—œä¿‚æ•¸': [autocorr[lag-1] for lag in significant_lags],
            'å¯èƒ½åŸå› ': [get_lag_interpretation(lag) for lag in significant_lags]
        })
        st.dataframe(sig_df)

def get_lag_interpretation(lag: int) -> str:
    """è§£é‡‹æ»¯å¾Œçš„å¯èƒ½åŸå› """
    if lag == 1:
        return "å¼·çƒˆçš„æ—¥é–“ä¾è³´æ€§"
    elif 2 <= lag <= 3:
        return "çŸ­æœŸè¨˜æ†¶æ•ˆæ‡‰"
    elif 6 <= lag <= 8:
        return "å¯èƒ½çš„é€±é€±æœŸæ€§"
    elif 13 <= lag <= 15:
        return "é›™é€±é€±æœŸæ€§"
    elif 28 <= lag <= 32:
        return "æœˆé€±æœŸæ€§"
    elif 88 <= lag <= 92:
        return "å­£åº¦é€±æœŸæ€§"
    else:
        return f"{lag}å¤©é€±æœŸæ€§"

def changepoint_detection(kpi_data: pd.DataFrame, kpi_name: str, fab_name: str):
    """è®Šé»æª¢æ¸¬"""
    st.subheader("âš¡ è®Šé»æª¢æ¸¬çµæœ")
    
    values = kpi_data['VALUE'].values
    dates = kpi_data['REPORT_TIME'].values
    
    if len(values) < 20:
        st.warning("âš ï¸ æ•¸æ“šé»å¤ªå°‘ï¼Œç„¡æ³•é€²è¡Œè®Šé»æª¢æ¸¬")
        return
    
    # ç°¡å–®çš„è®Šé»æª¢æ¸¬ç®—æ³•ï¼ˆåŸºæ–¼çµ±è¨ˆè®ŠåŒ–ï¼‰
    changepoints = detect_changepoints(values)
    
    # çµ±è¨ˆä¿¡æ¯
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("æª¢æ¸¬åˆ°è®Šé»æ•¸é‡", len(changepoints))
    
    with col2:
        if changepoints:
            last_change = max(changepoints)
            days_since = len(values) - last_change
            st.metric("è·é›¢æœ€è¿‘è®Šé»", f"{days_since}å¤©")
        else:
            st.metric("è·é›¢æœ€è¿‘è®Šé»", "ç„¡è®Šé»")
    
    with col3:
        stability_score = calculate_stability_score(values, changepoints)
        st.metric("ç©©å®šæ€§è©•åˆ†", f"{stability_score:.2f}")
    
    # è¦–è¦ºåŒ–
    fig = go.Figure()
    
    # åŸå§‹æ•¸æ“š
    fig.add_trace(go.Scatter(
        x=dates,
        y=values,
        mode='lines+markers',
        name='åŸå§‹æ•¸æ“š',
        line=dict(color='blue', width=2),
        marker=dict(size=4)
    ))
    
    # æ¨™è¨˜è®Šé»
    if changepoints:
        for cp in changepoints:
            fig.add_vline(
                x=dates[cp],
                line_dash="dash",
                line_color="red",
                annotation_text=f"è®Šé» {cp}"
            )
    
    # æ·»åŠ åˆ†æ®µå¹³å‡ç·š
    if changepoints:
        segments = [0] + changepoints + [len(values)]
        colors = ['green', 'orange', 'purple', 'brown', 'pink']
        
        for i in range(len(segments) - 1):
            start, end = segments[i], segments[i + 1]
            segment_mean = np.mean(values[start:end])
            
            fig.add_trace(go.Scatter(
                x=dates[start:end],
                y=[segment_mean] * (end - start),
                mode='lines',
                name=f'æ®µ {i+1} å¹³å‡',
                line=dict(color=colors[i % len(colors)], width=3)
            ))
    
    fig.update_layout(
        title=f"{fab_name} - {kpi_name} è®Šé»æª¢æ¸¬",
        xaxis_title="æ™‚é–“",
        yaxis_title="æ•¸å€¼",
        height=500
    )
    
    st.plotly_chart(fig)
    
    # è®Šé»è©³æƒ…
    if changepoints:
        st.subheader("ğŸ“‹ è®Šé»è©³ç´°ä¿¡æ¯")
        
        cp_details = []
        for i, cp in enumerate(changepoints):
            before_mean = np.mean(values[max(0, cp-10):cp])
            after_mean = np.mean(values[cp:min(len(values), cp+10)])
            change_magnitude = abs(after_mean - before_mean)
            change_direction = "ä¸Šå‡" if after_mean > before_mean else "ä¸‹é™"
            
            cp_details.append({
                'è®Šé»ä½ç½®': pd.to_datetime(dates[cp]).strftime('%Y-%m-%d'),
                'è®ŠåŒ–æ–¹å‘': change_direction,
                'è®ŠåŒ–å¹…åº¦': f"{change_magnitude:.2f}",
                'è®ŠåŒ–å‰å¹³å‡': f"{before_mean:.2f}",
                'è®ŠåŒ–å¾Œå¹³å‡': f"{after_mean:.2f}"
            })
        
        st.dataframe(pd.DataFrame(cp_details))

def detect_changepoints(values: np.ndarray, min_size: int = 5) -> List[int]:
    """ç°¡å–®çš„è®Šé»æª¢æ¸¬ç®—æ³•"""
    changepoints = []
    n = len(values)
    
    for i in range(min_size, n - min_size):
        # è¨ˆç®—å‰å¾Œçª—å£çš„çµ±è¨ˆå·®ç•°
        before = values[max(0, i-min_size):i]
        after = values[i:min(i+min_size, n)]
        
        if len(before) >= min_size and len(after) >= min_size:
            # ä½¿ç”¨ t æª¢é©—æª¢æ¸¬å‡å€¼è®ŠåŒ–
            t_stat, p_value = stats.ttest_ind(before, after)
            
            if p_value < 0.01:  # é¡¯è‘—æ€§æ°´æº–
                changepoints.append(i)
    
    # åˆä½µç›¸è¿‘çš„è®Šé»
    if changepoints:
        merged_changepoints = [changepoints[0]]
        for cp in changepoints[1:]:
            if cp - merged_changepoints[-1] > min_size:
                merged_changepoints.append(cp)
        return merged_changepoints
    
    return changepoints

def calculate_stability_score(values: np.ndarray, changepoints: List[int]) -> float:
    """è¨ˆç®—ç©©å®šæ€§è©•åˆ† (0-1, 1è¡¨ç¤ºæœ€ç©©å®š)"""
    # åŸºæ–¼è®Šé»æ•¸é‡å’Œæ³¢å‹•æ€§è¨ˆç®—
    volatility = np.std(values) / np.mean(values) if np.mean(values) != 0 else 1
    changepoint_penalty = len(changepoints) / len(values)
    
    stability = max(0, 1 - volatility - changepoint_penalty)
    return min(1, stability)

def time_series_decomposition(kpi_data: pd.DataFrame, kpi_name: str, fab_name: str):
    """æ™‚åºåˆ†è§£"""
    st.subheader("ğŸ”„ æ™‚åºåˆ†è§£çµæœ")
    
    values = kpi_data['VALUE'].values
    dates = kpi_data['REPORT_TIME'].values
    
    if len(values) < 30:
        st.warning("âš ï¸ æ•¸æ“šé»å¤ªå°‘ï¼Œç„¡æ³•é€²è¡Œæ™‚åºåˆ†è§£")
        return
    
    try:
        from statsmodels.tsa.seasonal import seasonal_decompose
        
        # åŸ·è¡Œåˆ†è§£
        decomposition = seasonal_decompose(values, model='additive', period=30, extrapolate_trend='freq')
        
        # å‰µå»ºå­åœ–
        fig = make_subplots(
            rows=4, cols=1,
            subplot_titles=('åŸå§‹æ•¸æ“š', 'è¶¨å‹¢', 'å­£ç¯€æ€§', 'æ®˜å·®'),
            vertical_spacing=0.08,
            shared_xaxes=True
        )
        
        # åŸå§‹æ•¸æ“š
        fig.add_trace(
            go.Scatter(x=dates, y=values, mode='lines', name='åŸå§‹', line=dict(color='blue')),
            row=1, col=1
        )
        
        # è¶¨å‹¢
        fig.add_trace(
            go.Scatter(x=dates, y=decomposition.trend, mode='lines', name='è¶¨å‹¢', line=dict(color='green')),
            row=2, col=1
        )
        
        # å­£ç¯€æ€§
        fig.add_trace(
            go.Scatter(x=dates, y=decomposition.seasonal, mode='lines', name='å­£ç¯€æ€§', line=dict(color='orange')),
            row=3, col=1
        )
        
        # æ®˜å·®
        fig.add_trace(
            go.Scatter(x=dates, y=decomposition.resid, mode='lines', name='æ®˜å·®', line=dict(color='red')),
            row=4, col=1
        )
        
        fig.update_layout(height=800, showlegend=False, title_text=f"{fab_name} - {kpi_name} æ™‚åºåˆ†è§£")
        st.plotly_chart(fig)
        
        # åˆ†è§£çµ±è¨ˆ
        col1, col2, col3 = st.columns(3)
        
        with col1:
            trend_strength = 1 - np.var(decomposition.resid) / np.var(values - decomposition.seasonal)
            st.metric("è¶¨å‹¢å¼·åº¦", f"{max(0, trend_strength):.3f}")
        
        with col2:
            seasonal_strength = 1 - np.var(decomposition.resid) / np.var(values - decomposition.trend)
            st.metric("å­£ç¯€æ€§å¼·åº¦", f"{max(0, seasonal_strength):.3f}")
        
        with col3:
            residual_std = np.std(decomposition.resid)
            st.metric("æ®˜å·®æ¨™æº–å·®", f"{residual_std:.3f}")
        
    except ImportError:
        st.error("âŒ éœ€è¦å®‰è£ statsmodels: pip install statsmodels")

def anomaly_pattern_analysis(fab_data: pd.DataFrame, selected_kpis: List[str], fab_name: str):
    """ç•°å¸¸æ¨¡å¼åˆ†æ"""
    st.subheader("ğŸ” ç•°å¸¸æ¨¡å¼åˆ†æçµæœ")
    
    # æº–å‚™æ•¸æ“š
    pivot_data = fab_data[fab_data['KPI'].isin(selected_kpis)].pivot_table(
        index='REPORT_TIME', columns='KPI', values='VALUE', aggfunc='mean'
    ).fillna(method='ffill').fillna(method='bfill')
    
    if pivot_data.empty:
        st.error("âŒ ç„¡æ³•å»ºç«‹æ•¸æ“šçŸ©é™£")
        return
    
    # æ¨™æº–åŒ–æ•¸æ“š
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(pivot_data.values)
    
    # PCA åˆ†æ
    pca = PCA(n_components=min(3, len(selected_kpis)))
    pca_result = pca.fit_transform(scaled_data)
    
    # ç•°å¸¸æª¢æ¸¬ï¼ˆåŸºæ–¼é¦¬æ°è·é›¢ï¼‰
    from scipy.spatial.distance import mahalanobis
    
    cov_matrix = np.cov(scaled_data.T)
    mean_vector = np.mean(scaled_data, axis=0)
    
    try:
        inv_cov_matrix = np.linalg.inv(cov_matrix)
        mahalanobis_distances = [
            mahalanobis(row, mean_vector, inv_cov_matrix) for row in scaled_data
        ]
    except:
        # å¦‚æœå”æ–¹å·®çŸ©é™£ä¸å¯é€†ï¼Œä½¿ç”¨æ­æ°è·é›¢
        mahalanobis_distances = [
            np.linalg.norm(row - mean_vector) for row in scaled_data
        ]
    
    # è­˜åˆ¥ç•°å¸¸é»
    threshold = np.percentile(mahalanobis_distances, 95)  # å‰5%ä½œç‚ºç•°å¸¸
    anomaly_indices = np.where(np.array(mahalanobis_distances) > threshold)[0]
    
    # çµæœå±•ç¤º
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("åˆ†æ KPI æ•¸é‡", len(selected_kpis))
    
    with col2:
        st.metric("ç•°å¸¸æ™‚é–“é»", len(anomaly_indices))
    
    with col3:
        anomaly_rate = len(anomaly_indices) / len(pivot_data) * 100
        st.metric("ç•°å¸¸æ¯”ä¾‹", f"{anomaly_rate:.2f}%")
    
    # PCA å¯è¦–åŒ–
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('PCA å‰å…©å€‹ä¸»æˆåˆ†', 'PCA è²¢ç»ç‡', 'ç•°å¸¸åˆ†æ•¸æ™‚åº', 'KPI ç›¸é—œæ€§ç†±åœ–'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # PCA æ•£é»åœ–
    colors = ['red' if i in anomaly_indices else 'blue' for i in range(len(pca_result))]
    fig.add_trace(
        go.Scatter(x=pca_result[:, 0], y=pca_result[:, 1], 
                   mode='markers', marker=dict(color=colors),
                   name='æ•¸æ“šé»'),
        row=1, col=1
    )
    
    # PCA è²¢ç»ç‡
    fig.add_trace(
        go.Bar(x=[f'PC{i+1}' for i in range(len(pca.explained_variance_ratio_))],
               y=pca.explained_variance_ratio_, name='è²¢ç»ç‡'),
        row=1, col=2
    )
    
    # ç•°å¸¸åˆ†æ•¸æ™‚åº
    fig.add_trace(
        go.Scatter(x=pivot_data.index, y=mahalanobis_distances,
                   mode='lines+markers', name='ç•°å¸¸åˆ†æ•¸'),
        row=2, col=1
    )
    fig.add_hline(y=threshold, line_dash="dash", line_color="red", row=2, col=1)
    
    # ç›¸é—œæ€§ç†±åœ–
    corr_matrix = pivot_data.corr()
    fig.add_trace(
        go.Heatmap(z=corr_matrix.values, x=corr_matrix.columns, y=corr_matrix.index,
                   colorscale='RdYlBu', zmid=0),
        row=2, col=2
    )
    
    fig.update_layout(height=800, showlegend=False)
    st.plotly_chart(fig)
    
    # ç•°å¸¸æ™‚é–“é»è©³æƒ…
    if len(anomaly_indices) > 0:
        st.subheader("ğŸ“‹ ç•°å¸¸æ™‚é–“é»è©³æƒ…")
        
        anomaly_details = []
        for idx in anomaly_indices[:10]:  # åªé¡¯ç¤ºå‰10å€‹
            date = pivot_data.index[idx]
            anomaly_score = mahalanobis_distances[idx]
            
            # æ‰¾å‡ºè©²æ™‚é–“é»æœ€ç•°å¸¸çš„KPI
            row_data = pivot_data.iloc[idx]
            z_scores = (row_data - pivot_data.mean()) / pivot_data.std()
            most_anomalous_kpi = z_scores.abs().idxmax()
            
            anomaly_details.append({
                'æ™‚é–“': pd.to_datetime(date).strftime('%Y-%m-%d'),
                'ç•°å¸¸åˆ†æ•¸': f"{anomaly_score:.3f}",
                'æœ€ç•°å¸¸KPI': most_anomalous_kpi,
                'Z-Score': f"{z_scores[most_anomalous_kpi]:.3f}"
            })
        
        st.dataframe(pd.DataFrame(anomaly_details))

# æ–°å¢å…¶ä»–é é¢çš„ä½”ä½å‡½æ•¸
def anomaly_trend_analysis_page():
    """ç•°å¸¸è¶¨å‹¢åˆ†æé é¢"""
    st.header("ğŸ“Š ç•°å¸¸è¶¨å‹¢åˆ†æ")
    
    if st.session_state.fab_data is None:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³æ•¸æ“šä¸¦é¸æ“‡ FAB")
        return
    
    st.info("ğŸ”§ ç•°å¸¸è¶¨å‹¢åˆ†æåŠŸèƒ½é–‹ç™¼ä¸­...")
    # é€™è£¡å¯ä»¥å¯¦ç¾æ›´å¤šåŠŸèƒ½ï¼Œå¦‚ï¼š
    # - ç•°å¸¸é »ç‡è¶¨å‹¢
    # - ç•°å¸¸é¡å‹åˆ†é¡
    # - ç•°å¸¸æŒçºŒæ™‚é–“åˆ†æ
    # - ç•°å¸¸å½±éŸ¿ç¯„åœåˆ†æ