import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from typing import Dict, List, Tuple, Optional
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from matplotlib_utils import render_matplotlib_figure, create_isolation_forest_plot, create_dbscan_plot, create_anomaly_plot
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def isolation_forest_analysis_page():
    """Isolation Forest ç•°å¸¸æª¢æ¸¬é é¢"""
    st.header("ğŸŒ² Isolation Forest ç•°å¸¸æª¢æ¸¬")
    
    if st.session_state.fab_data is None:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³æ•¸æ“šä¸¦é¸æ“‡ FAB")
        st.info("ğŸ’¡ è«‹å…ˆå¾å·¦å´é¸å–®é¸æ“‡ã€ŒKPI å¿«é€Ÿåˆ†æã€è¼‰å…¥æ•¸æ“š")
        return
    
    fab_data = st.session_state.fab_data
    selected_fab = st.session_state.selected_fab
    available_kpis = st.session_state.available_kpis
    
    # é¡¯ç¤ºç•¶å‰é¸æ“‡
    st.info(f"ğŸ­ ç•¶å‰ FAB: **{selected_fab}** | ğŸ“Š ç•¶å‰ KPI: **{st.session_state.selected_kpi}**")
    
    st.subheader("âš™ï¸ æ¨¡å‹åƒæ•¸è¨­å®š")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        selected_kpi = st.selectbox(
            "é¸æ“‡ KPI:",
            options=available_kpis,
            index=available_kpis.index(st.session_state.selected_kpi) if st.session_state.selected_kpi in available_kpis else 0
        )
    
    with col2:
        contamination = st.slider(
            "æ±¡æŸ“ç‡ (ç•°å¸¸æ¯”ä¾‹):",
            min_value=0.01, max_value=0.3, value=0.1, step=0.01,
            help="é æœŸç•°å¸¸é»ä½”ç¸½æ•¸æ“šçš„æ¯”ä¾‹"
        )
    
    with col3:
        n_estimators = st.slider(
            "æ¨¹çš„æ•¸é‡:",
            min_value=50, max_value=300, value=100, step=10,
            help="éš¨æ©Ÿæ£®æ—ä¸­æ¨¹çš„æ•¸é‡ï¼Œæ›´å¤šçš„æ¨¹é€šå¸¸æä¾›æ›´å¥½çš„æ€§èƒ½"
        )
    
    with col4:
        max_features = st.slider(
            "æœ€å¤§ç‰¹å¾µæ•¸:",
            min_value=1, max_value=10, value=1, step=1,
            help="æ¯æ£µæ¨¹ä½¿ç”¨çš„æœ€å¤§ç‰¹å¾µæ•¸"
        )
    
    # æ™‚é–“çª—å£ç‰¹å¾µ
    st.subheader("ğŸ“Š ç‰¹å¾µå·¥ç¨‹è¨­å®š")
    col1, col2 = st.columns(2)
    
    with col1:
        use_time_features = st.checkbox("ä½¿ç”¨æ™‚é–“ç‰¹å¾µ", value=True, help="åŒ…æ‹¬å°æ™‚ã€æ˜ŸæœŸç­‰æ™‚é–“ç‰¹å¾µ")
        window_size = st.slider("æ»¾å‹•çª—å£å¤§å°:", 3, 30, 7, help="ç”¨æ–¼è¨ˆç®—æ»¾å‹•çµ±è¨ˆç‰¹å¾µçš„çª—å£å¤§å°")
    
    with col2:
        use_lag_features = st.checkbox("ä½¿ç”¨æ»¯å¾Œç‰¹å¾µ", value=True, help="åŒ…æ‹¬å‰å¹¾æœŸçš„æ•¸å€¼ä½œç‚ºç‰¹å¾µ")
        n_lags = st.slider("æ»¯å¾ŒæœŸæ•¸:", 1, 10, 3, help="åŒ…æ‹¬å¤šå°‘å€‹æ»¯å¾ŒæœŸçš„ç‰¹å¾µ")
    
    if st.button("ğŸ” åŸ·è¡Œ Isolation Forest åˆ†æ", type="primary"):
        kpi_data = fab_data[fab_data['KPI'] == selected_kpi].copy()
        kpi_data = kpi_data.sort_values('REPORT_TIME')
        
        if len(kpi_data) < 20:
            st.error("âŒ æ•¸æ“šé»ä¸è¶³ï¼Œè‡³å°‘éœ€è¦20å€‹æ•¸æ“šé»")
            return
        
        # åŸ·è¡Œ Isolation Forest åˆ†æ
        results = perform_isolation_forest_analysis(
            kpi_data, contamination, n_estimators, max_features,
            use_time_features, use_lag_features, window_size, n_lags
        )
        
        # é¡¯ç¤ºçµæœ
        display_isolation_forest_results(results, selected_kpi, selected_fab)
        
        # é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§åˆ†æ
        display_feature_importance_analysis(results)

def dbscan_analysis_page():
    """DBSCAN èšé¡ç•°å¸¸æª¢æ¸¬é é¢"""
    st.header("ğŸ¯ DBSCAN èšé¡ç•°å¸¸æª¢æ¸¬")
    
    if st.session_state.fab_data is None:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³æ•¸æ“šä¸¦é¸æ“‡ FAB")
        st.info("ğŸ’¡ è«‹å…ˆå¾å·¦å´é¸å–®é¸æ“‡ã€ŒKPI å¿«é€Ÿåˆ†æã€è¼‰å…¥æ•¸æ“š")
        return
    
    fab_data = st.session_state.fab_data
    selected_fab = st.session_state.selected_fab
    available_kpis = st.session_state.available_kpis
    
    # é¡¯ç¤ºç•¶å‰é¸æ“‡
    st.info(f"ğŸ­ ç•¶å‰ FAB: **{selected_fab}** | ğŸ“Š ç•¶å‰ KPI: **{st.session_state.selected_kpi}**")
    
    st.subheader("âš™ï¸ èšé¡åƒæ•¸è¨­å®š")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        selected_kpi = st.selectbox(
            "é¸æ“‡ KPI:",
            options=available_kpis,
            index=available_kpis.index(st.session_state.selected_kpi) if st.session_state.selected_kpi in available_kpis else 0
        )
    
    with col2:
        eps = st.slider(
            "eps (é„°åŸŸåŠå¾‘):",
            min_value=0.1, max_value=5.0, value=0.5, step=0.1,
            help="å®šç¾©é„°åŸŸçš„åŠå¾‘ï¼Œè¼ƒå°çš„å€¼æœƒç”¢ç”Ÿæ›´å¤šçš„å°èšé¡"
        )
    
    with col3:
        min_samples = st.slider(
            "æœ€å°æ¨£æœ¬æ•¸:",
            min_value=3, max_value=20, value=5, step=1,
            help="å½¢æˆèšé¡æ‰€éœ€çš„æœ€å°æ¨£æœ¬æ•¸"
        )
    
    with col4:
        metric = st.selectbox(
            "è·é›¢åº¦é‡:",
            ["euclidean", "manhattan", "cosine"],
            help="è¨ˆç®—é»ä¹‹é–“è·é›¢çš„æ–¹æ³•"
        )
    
    # ç‰¹å¾µé¸æ“‡
    st.subheader("ğŸ“Š ç‰¹å¾µé¸æ“‡")
    col1, col2 = st.columns(2)
    
    with col1:
        use_value_features = st.checkbox("ä½¿ç”¨æ•¸å€¼ç‰¹å¾µ", value=True)
        use_statistical_features = st.checkbox("ä½¿ç”¨çµ±è¨ˆç‰¹å¾µ", value=True, help="åŒ…æ‹¬æ»¾å‹•å‡å€¼ã€æ¨™æº–å·®ç­‰")
    
    with col2:
        use_time_features = st.checkbox("ä½¿ç”¨æ™‚é–“ç‰¹å¾µ", value=False, help="åŒ…æ‹¬å°æ™‚ã€æ—¥æœŸç­‰")
        window_size = st.slider("çµ±è¨ˆçª—å£å¤§å°:", 3, 30, 7)
    
    if st.button("ğŸ” åŸ·è¡Œ DBSCAN åˆ†æ", type="primary"):
        kpi_data = fab_data[fab_data['KPI'] == selected_kpi].copy()
        kpi_data = kpi_data.sort_values('REPORT_TIME')
        
        if len(kpi_data) < min_samples * 2:
            st.error(f"âŒ æ•¸æ“šé»ä¸è¶³ï¼Œè‡³å°‘éœ€è¦ {min_samples * 2} å€‹æ•¸æ“šé»")
            return
        
        # åŸ·è¡Œ DBSCAN åˆ†æ
        results = perform_dbscan_analysis(
            kpi_data, eps, min_samples, metric,
            use_value_features, use_statistical_features, 
            use_time_features, window_size
        )
        
        # é¡¯ç¤ºçµæœ
        display_dbscan_results(results, selected_kpi, selected_fab)
        
        # é¡¯ç¤ºèšé¡åˆ†æ
        display_cluster_analysis(results)

def ensemble_anomaly_detection_page():
    """é›†æˆç•°å¸¸æª¢æ¸¬é é¢"""
    st.header("ğŸª é›†æˆç•°å¸¸æª¢æ¸¬")
    
    if st.session_state.fab_data is None:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³æ•¸æ“šä¸¦é¸æ“‡ FAB")
        st.info("ğŸ’¡ è«‹å…ˆå¾å·¦å´é¸å–®é¸æ“‡ã€ŒKPI å¿«é€Ÿåˆ†æã€è¼‰å…¥æ•¸æ“š")
        return
    
    fab_data = st.session_state.fab_data
    selected_fab = st.session_state.selected_fab
    available_kpis = st.session_state.available_kpis
    
    # é¡¯ç¤ºç•¶å‰é¸æ“‡
    st.info(f"ğŸ­ ç•¶å‰ FAB: **{selected_fab}** | ğŸ“Š ç•¶å‰ KPI: **{st.session_state.selected_kpi}**")
    
    st.subheader("ğŸ¯ é¸æ“‡æª¢æ¸¬æ–¹æ³•")
    
    col1, col2 = st.columns(2)
    
    with col1:
        selected_kpi = st.selectbox(
            "é¸æ“‡ KPI:",
            options=available_kpis,
            index=available_kpis.index(st.session_state.selected_kpi) if st.session_state.selected_kpi in available_kpis else 0
        )
        
        methods = st.multiselect(
            "é¸æ“‡æª¢æ¸¬æ–¹æ³•:",
            ["Z-Score", "IQR", "Isolation Forest", "DBSCAN", "LOF", "One-Class SVM"],
            default=["Z-Score", "IQR", "Isolation Forest"],
            help="é¸æ“‡å¤šç¨®æ–¹æ³•é€²è¡Œé›†æˆæª¢æ¸¬"
        )
    
    with col2:
        ensemble_strategy = st.selectbox(
            "é›†æˆç­–ç•¥:",
            ["Voting (æŠ•ç¥¨)", "Union (ä¸¦é›†)", "Intersection (äº¤é›†)", "Weighted (åŠ æ¬Š)"],
            help="å¦‚ä½•çµåˆå¤šç¨®æ–¹æ³•çš„çµæœ"
        )
        
        threshold_percentile = st.slider(
            "ç•°å¸¸é–¾å€¼ç™¾åˆ†ä½:",
            min_value=90, max_value=99, value=95,
            help="ç•°å¸¸é»çš„ç™¾åˆ†ä½é–¾å€¼"
        )
    
    if len(methods) < 2:
        st.warning("âš ï¸ è«‹è‡³å°‘é¸æ“‡å…©ç¨®æª¢æ¸¬æ–¹æ³•")
        return
    
    if st.button("ğŸ” åŸ·è¡Œé›†æˆç•°å¸¸æª¢æ¸¬", type="primary"):
        kpi_data = fab_data[fab_data['KPI'] == selected_kpi].copy()
        kpi_data = kpi_data.sort_values('REPORT_TIME')
        
        if len(kpi_data) < 20:
            st.error("âŒ æ•¸æ“šé»ä¸è¶³ï¼Œè‡³å°‘éœ€è¦20å€‹æ•¸æ“šé»")
            return
        
        # åŸ·è¡Œé›†æˆç•°å¸¸æª¢æ¸¬
        results = perform_ensemble_anomaly_detection(
            kpi_data, methods, ensemble_strategy, threshold_percentile
        )
        
        # é¡¯ç¤ºçµæœ
        display_ensemble_results(results, selected_kpi, selected_fab, methods)

def perform_isolation_forest_analysis(kpi_data: pd.DataFrame, contamination: float,
                                     n_estimators: int, max_features: int,
                                     use_time_features: bool, use_lag_features: bool,
                                     window_size: int, n_lags: int) -> Dict:
    """åŸ·è¡Œ Isolation Forest åˆ†æ"""
    
    # ç‰¹å¾µå·¥ç¨‹
    features_df = create_features_for_isolation_forest(
        kpi_data, use_time_features, use_lag_features, window_size, n_lags
    )
    
    # æ¨™æº–åŒ–ç‰¹å¾µ
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features_df.values)
    
    # è¨“ç·´ Isolation Forest
    iso_forest = IsolationForest(
        contamination=contamination,
        n_estimators=n_estimators,
        max_features=max_features,
        random_state=42,
        n_jobs=-1
    )
    
    predictions = iso_forest.fit_predict(features_scaled)
    anomaly_scores = iso_forest.decision_function(features_scaled)
    
    # æ‰¾å‡ºç•°å¸¸é»
    anomaly_mask = predictions == -1
    anomaly_indices = np.where(anomaly_mask)[0]
    
    return {
        'kpi_data': kpi_data,
        'features_df': features_df,
        'features_scaled': features_scaled,
        'predictions': predictions,
        'anomaly_scores': anomaly_scores,
        'anomaly_indices': anomaly_indices,
        'model': iso_forest,
        'scaler': scaler,
        'contamination': contamination
    }

def perform_dbscan_analysis(kpi_data: pd.DataFrame, eps: float, min_samples: int,
                          metric: str, use_value_features: bool, 
                          use_statistical_features: bool, use_time_features: bool,
                          window_size: int) -> Dict:
    """åŸ·è¡Œ DBSCAN åˆ†æ"""
    
    # ç‰¹å¾µå·¥ç¨‹
    features_df = create_features_for_dbscan(
        kpi_data, use_value_features, use_statistical_features, 
        use_time_features, window_size
    )
    
    # æ¨™æº–åŒ–ç‰¹å¾µ
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features_df.values)
    
    # åŸ·è¡Œ DBSCAN
    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, n_jobs=-1)
    labels = dbscan.fit_predict(features_scaled)
    
    # è­˜åˆ¥å™ªè²é»ï¼ˆç•°å¸¸é»ï¼‰
    anomaly_mask = labels == -1
    anomaly_indices = np.where(anomaly_mask)[0]
    
    return {
        'kpi_data': kpi_data,
        'features_df': features_df,
        'features_scaled': features_scaled,
        'labels': labels,
        'anomaly_indices': anomaly_indices,
        'model': dbscan,
        'scaler': scaler,
        'eps': eps,
        'min_samples': min_samples,
        'n_clusters': len(set(labels)) - (1 if -1 in labels else 0)
    }

def perform_ensemble_anomaly_detection(kpi_data: pd.DataFrame, methods: List[str],
                                     ensemble_strategy: str, threshold_percentile: int) -> Dict:
    """åŸ·è¡Œé›†æˆç•°å¸¸æª¢æ¸¬"""
    
    values = kpi_data['VALUE'].values
    results = {}
    anomaly_scores_dict = {}
    
    # å°æ¯ç¨®æ–¹æ³•åŸ·è¡Œæª¢æ¸¬
    for method in methods:
        if method == "Z-Score":
            mean_val = np.mean(values)
            std_val = np.std(values)
            scores = np.abs((values - mean_val) / std_val)
            threshold = 2.0
            anomalies = np.where(scores > threshold)[0]
            
        elif method == "IQR":
            Q1 = np.percentile(values, 25)
            Q3 = np.percentile(values, 75)
            IQR = Q3 - Q1
            scores = np.maximum((Q1 - values) / IQR, (values - Q3) / IQR)
            threshold = 1.5
            anomalies = np.where(scores > threshold)[0]
            
        elif method == "Isolation Forest":
            # ç°¡åŒ–çš„ Isolation Forest
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            features = values.reshape(-1, 1)
            predictions = iso_forest.fit_predict(features)
            scores = -iso_forest.decision_function(features)  # è½‰æ›ç‚ºæ­£çš„ç•°å¸¸åˆ†æ•¸
            anomalies = np.where(predictions == -1)[0]
            
        elif method == "LOF":
            lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
            predictions = lof.fit_predict(values.reshape(-1, 1))
            scores = -lof.negative_outlier_factor_
            anomalies = np.where(predictions == -1)[0]
            
        elif method == "One-Class SVM":
            svm = OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')
            predictions = svm.fit_predict(values.reshape(-1, 1))
            scores = -svm.decision_function(values.reshape(-1, 1)).flatten()
            anomalies = np.where(predictions == -1)[0]
            
        else:
            continue
        
        results[method] = {
            'anomalies': anomalies,
            'scores': scores
        }
        anomaly_scores_dict[method] = scores
    
    # é›†æˆç­–ç•¥
    ensemble_anomalies = combine_anomaly_results(
        results, ensemble_strategy, len(values), threshold_percentile
    )
    
    return {
        'kpi_data': kpi_data,
        'individual_results': results,
        'ensemble_anomalies': ensemble_anomalies,
        'anomaly_scores_dict': anomaly_scores_dict,
        'ensemble_strategy': ensemble_strategy
    }

def create_features_for_isolation_forest(kpi_data: pd.DataFrame, use_time_features: bool,
                                        use_lag_features: bool, window_size: int, n_lags: int) -> pd.DataFrame:
    """ç‚º Isolation Forest å‰µå»ºç‰¹å¾µ"""
    features = pd.DataFrame()
    
    # åŸºç¤ç‰¹å¾µ
    features['value'] = kpi_data['VALUE']
    
    # æ»¾å‹•çµ±è¨ˆç‰¹å¾µ
    features['rolling_mean'] = kpi_data['VALUE'].rolling(window_size).mean()
    features['rolling_std'] = kpi_data['VALUE'].rolling(window_size).std()
    features['rolling_min'] = kpi_data['VALUE'].rolling(window_size).min()
    features['rolling_max'] = kpi_data['VALUE'].rolling(window_size).max()
    
    # æ»¯å¾Œç‰¹å¾µ
    if use_lag_features:
        for lag in range(1, n_lags + 1):
            features[f'lag_{lag}'] = kpi_data['VALUE'].shift(lag)
    
    # æ™‚é–“ç‰¹å¾µ
    if use_time_features:
        kpi_data['REPORT_TIME'] = pd.to_datetime(kpi_data['REPORT_TIME'])
        features['hour'] = kpi_data['REPORT_TIME'].dt.hour
        features['day_of_week'] = kpi_data['REPORT_TIME'].dt.dayofweek
        features['day_of_month'] = kpi_data['REPORT_TIME'].dt.day
        features['month'] = kpi_data['REPORT_TIME'].dt.month
    
    # è®ŠåŒ–ç‰¹å¾µ
    features['diff'] = kpi_data['VALUE'].diff()
    features['pct_change'] = kpi_data['VALUE'].pct_change()
    
    # å¡«è£œç¼ºå¤±å€¼
    features = features.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    return features

def create_features_for_dbscan(kpi_data: pd.DataFrame, use_value_features: bool,
                              use_statistical_features: bool, use_time_features: bool,
                              window_size: int) -> pd.DataFrame:
    """ç‚º DBSCAN å‰µå»ºç‰¹å¾µ"""
    features = pd.DataFrame()
    
    if use_value_features:
        features['value'] = kpi_data['VALUE']
        features['value_normalized'] = (kpi_data['VALUE'] - kpi_data['VALUE'].mean()) / kpi_data['VALUE'].std()
    
    if use_statistical_features:
        features['rolling_mean'] = kpi_data['VALUE'].rolling(window_size).mean()
        features['rolling_std'] = kpi_data['VALUE'].rolling(window_size).std()
        features['rolling_skew'] = kpi_data['VALUE'].rolling(window_size).skew()
        features['z_score'] = (kpi_data['VALUE'] - kpi_data['VALUE'].rolling(window_size).mean()) / kpi_data['VALUE'].rolling(window_size).std()
    
    if use_time_features:
        kpi_data['REPORT_TIME'] = pd.to_datetime(kpi_data['REPORT_TIME'])
        features['hour'] = kpi_data['REPORT_TIME'].dt.hour
        features['day_of_week'] = kpi_data['REPORT_TIME'].dt.dayofweek
    
    # å¡«è£œç¼ºå¤±å€¼
    features = features.fillna(method='ffill').fillna(method='bfill').fillna(0)
    
    return features

def combine_anomaly_results(results: Dict, strategy: str, n_samples: int, threshold_percentile: int) -> np.ndarray:
    """çµåˆå¤šç¨®ç•°å¸¸æª¢æ¸¬æ–¹æ³•çš„çµæœ"""
    
    if strategy == "Union (ä¸¦é›†)":
        # ä»»ä½•æ–¹æ³•æª¢æ¸¬åˆ°çš„ç•°å¸¸éƒ½ç®—ç•°å¸¸
        all_anomalies = set()
        for method_result in results.values():
            all_anomalies.update(method_result['anomalies'])
        return np.array(list(all_anomalies))
    
    elif strategy == "Intersection (äº¤é›†)":
        # æ‰€æœ‰æ–¹æ³•éƒ½æª¢æ¸¬åˆ°çš„ç•°å¸¸æ‰ç®—ç•°å¸¸
        if not results:
            return np.array([])
        
        common_anomalies = set(results[list(results.keys())[0]]['anomalies'])
        for method_result in list(results.values())[1:]:
            common_anomalies = common_anomalies.intersection(set(method_result['anomalies']))
        return np.array(list(common_anomalies))
    
    elif strategy == "Voting (æŠ•ç¥¨)":
        # å¤šæ•¸æŠ•ç¥¨
        vote_counts = np.zeros(n_samples)
        for method_result in results.values():
            vote_counts[method_result['anomalies']] += 1
        
        threshold_votes = len(results) // 2 + 1
        return np.where(vote_counts >= threshold_votes)[0]
    
    elif strategy == "Weighted (åŠ æ¬Š)":
        # åŸºæ–¼ç•°å¸¸åˆ†æ•¸çš„åŠ æ¬Šå¹³å‡
        weighted_scores = np.zeros(n_samples)
        total_weight = 0
        
        for method, method_result in results.items():
            # ç°¡å–®çš„æ¬Šé‡åˆ†é…
            weight = 1.0
            normalized_scores = (method_result['scores'] - np.min(method_result['scores'])) / (np.max(method_result['scores']) - np.min(method_result['scores']) + 1e-10)
            weighted_scores += weight * normalized_scores
            total_weight += weight
        
        weighted_scores /= total_weight
        threshold = np.percentile(weighted_scores, threshold_percentile)
        return np.where(weighted_scores > threshold)[0]
    
    return np.array([])

def display_isolation_forest_results(results: Dict, kpi_name: str, fab_name: str):
    """é¡¯ç¤º Isolation Forest çµæœ"""
    
    # çµ±è¨ˆæ‘˜è¦
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ•¸æ“šé»ç¸½æ•¸", len(results['kpi_data']))
    
    with col2:
        st.metric("ç•°å¸¸é»æ•¸é‡", len(results['anomaly_indices']))
    
    with col3:
        anomaly_rate = len(results['anomaly_indices']) / len(results['kpi_data']) * 100
        st.metric("ç•°å¸¸ç‡", f"{anomaly_rate:.2f}%")
    
    with col4:
        st.metric("ç‰¹å¾µæ•¸é‡", results['features_df'].shape[1])
    
    # è¦–è¦ºåŒ–
    kpi_data = results['kpi_data']
    dates = pd.to_datetime(kpi_data['REPORT_TIME'])
    values = kpi_data['VALUE'].values
    
    fig = create_isolation_forest_plot(
        dates=dates,
        values=values,
        anomaly_scores=results['anomaly_scores'],
        predictions=results['predictions'],
        contamination=results['contamination'],
        title=f"{fab_name} - {kpi_name}"
    )
    
    render_matplotlib_figure(fig)

def display_dbscan_results(results: Dict, kpi_name: str, fab_name: str):
    """é¡¯ç¤º DBSCAN çµæœ"""
    
    # çµ±è¨ˆæ‘˜è¦
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ•¸æ“šé»ç¸½æ•¸", len(results['kpi_data']))
    
    with col2:
        st.metric("èšé¡æ•¸é‡", results['n_clusters'])
    
    with col3:
        st.metric("å™ªè²é»æ•¸é‡", len(results['anomaly_indices']))
    
    with col4:
        noise_rate = len(results['anomaly_indices']) / len(results['kpi_data']) * 100
        st.metric("å™ªè²ç‡", f"{noise_rate:.2f}%")
    
    # è¦–è¦ºåŒ–
    kpi_data = results['kpi_data']
    dates = pd.to_datetime(kpi_data['REPORT_TIME'])
    values = kpi_data['VALUE'].values
    
    fig = create_dbscan_plot(
        dates=dates,
        values=values,
        labels=results['labels'],
        eps=results['eps'],
        min_samples=results['min_samples'],
        title=f"{fab_name} - {kpi_name}"
    )
    
    render_matplotlib_figure(fig)

def display_ensemble_results(results: Dict, kpi_name: str, fab_name: str, methods: List[str]):
    """é¡¯ç¤ºé›†æˆç•°å¸¸æª¢æ¸¬çµæœ"""
    
    # çµ±è¨ˆæ‘˜è¦
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æª¢æ¸¬æ–¹æ³•æ•¸", len(methods))
    
    with col2:
        st.metric("æ•¸æ“šé»ç¸½æ•¸", len(results['kpi_data']))
    
    with col3:
        st.metric("é›†æˆç•°å¸¸é»", len(results['ensemble_anomalies']))
    
    with col4:
        ensemble_rate = len(results['ensemble_anomalies']) / len(results['kpi_data']) * 100
        st.metric("é›†æˆç•°å¸¸ç‡", f"{ensemble_rate:.2f}%")
    
    # æ¯”è¼ƒè¦–è¦ºåŒ–
    kpi_data = results['kpi_data']
    dates = pd.to_datetime(kpi_data['REPORT_TIME'])
    values = kpi_data['VALUE'].values
    
    # å‰µå»ºæ¯”è¼ƒåœ–è¡¨
    methods_results = {}
    for method, result in results['individual_results'].items():
        methods_results[method] = {
            'anomalies': result['anomalies'],
            'scores': result['scores']
        }
    
    from matplotlib_utils import create_comparison_plot
    fig = create_comparison_plot(
        dates=dates,
        values=values,
        methods_results=methods_results,
        title=f"{fab_name} - {kpi_name}"
    )
    
    render_matplotlib_figure(fig)
    
    # æ–¹æ³•ä¸€è‡´æ€§åˆ†æ
    st.subheader("ğŸ“Š æ–¹æ³•ä¸€è‡´æ€§åˆ†æ")
    
    consistency_data = []
    for i, method1 in enumerate(methods):
        for j, method2 in enumerate(methods):
            if i < j:
                anomalies1 = set(results['individual_results'][method1]['anomalies'])
                anomalies2 = set(results['individual_results'][method2]['anomalies'])
                
                intersection = len(anomalies1.intersection(anomalies2))
                union = len(anomalies1.union(anomalies2))
                jaccard = intersection / union if union > 0 else 0
                
                consistency_data.append({
                    'æ–¹æ³•1': method1,
                    'æ–¹æ³•2': method2,
                    'äº¤é›†æ•¸é‡': intersection,
                    'ä¸¦é›†æ•¸é‡': union,
                    'Jaccardç›¸ä¼¼åº¦': f"{jaccard:.3f}"
                })
    
    if consistency_data:
        st.dataframe(pd.DataFrame(consistency_data))

def display_feature_importance_analysis(results: Dict):
    """é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§åˆ†æ"""
    st.subheader("ğŸ“ˆ ç‰¹å¾µåˆ†æ")
    
    feature_names = results['features_df'].columns.tolist()
    
    # é¡¯ç¤ºç‰¹å¾µçµ±è¨ˆ
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ç‰¹å¾µçµ±è¨ˆæ‘˜è¦:**")
        st.dataframe(results['features_df'].describe())
    
    with col2:
        st.write("**ç•°å¸¸é»çš„ç‰¹å¾µç‰¹æ€§:**")
        if len(results['anomaly_indices']) > 0:
            anomaly_features = results['features_df'].iloc[results['anomaly_indices']]
            normal_features = results['features_df'].iloc[~results['features_df'].index.isin(results['anomaly_indices'])]
            
            comparison_stats = []
            for col in feature_names:
                comparison_stats.append({
                    'ç‰¹å¾µ': col,
                    'ç•°å¸¸é»å‡å€¼': f"{anomaly_features[col].mean():.3f}",
                    'æ­£å¸¸é»å‡å€¼': f"{normal_features[col].mean():.3f}",
                    'å·®ç•°': f"{abs(anomaly_features[col].mean() - normal_features[col].mean()):.3f}"
                })
            
            st.dataframe(pd.DataFrame(comparison_stats))

def display_cluster_analysis(results: Dict):
    """é¡¯ç¤ºèšé¡åˆ†æ"""
    st.subheader("ğŸ¯ èšé¡åˆ†æè©³æƒ…")
    
    labels = results['labels']
    unique_labels = np.unique(labels)
    
    cluster_stats = []
    for label in unique_labels:
        mask = labels == label
        cluster_size = np.sum(mask)
        
        if label == -1:
            cluster_stats.append({
                'èšé¡ID': 'å™ªè²é»',
                'é»æ•¸é‡': cluster_size,
                'ç™¾åˆ†æ¯”': f"{cluster_size / len(labels) * 100:.1f}%",
                'é¡å‹': 'ç•°å¸¸'
            })
        else:
            cluster_stats.append({
                'èšé¡ID': f'èšé¡ {label}',
                'é»æ•¸é‡': cluster_size,
                'ç™¾åˆ†æ¯”': f"{cluster_size / len(labels) * 100:.1f}%",
                'é¡å‹': 'æ­£å¸¸'
            })
    
    cluster_df = pd.DataFrame(cluster_stats)
    cluster_df = cluster_df.sort_values('é»æ•¸é‡', ascending=False)
    st.dataframe(cluster_df)