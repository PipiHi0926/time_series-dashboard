#!/usr/bin/env python3
"""
æ¸¬è©¦è…³æœ¬ï¼šé©—è­‰ matplotlib è½‰æ›æ˜¯å¦æˆåŠŸ
Test script to verify matplotlib conversion is successful
"""

import sys
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

def generate_test_data():
    """ç”Ÿæˆæ¸¬è©¦æ•¸æ“š"""
    # ç”Ÿæˆæ™‚é–“åºåˆ—
    dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')
    n_points = len(dates)
    
    # ç”ŸæˆåŸºç¤æ•¸æ“š
    base_trend = np.linspace(100, 120, n_points)
    seasonal = 10 * np.sin(2 * np.pi * np.arange(n_points) / 30)  # æœˆé€±æœŸ
    noise = np.random.normal(0, 2, n_points)
    
    # æ·»åŠ ä¸€äº›ç•°å¸¸é»
    anomaly_indices = np.random.choice(n_points, size=int(0.05 * n_points), replace=False)
    anomaly_values = np.random.normal(0, 15, len(anomaly_indices))
    
    # çµ„åˆæ•¸æ“š
    values = base_trend + seasonal + noise
    values[anomaly_indices] += anomaly_values
    
    # å‰µå»º DataFrame
    test_data = pd.DataFrame({
        'REPORT_TIME': dates,
        'FAB': ['FAB01'] * n_points,
        'KPI': ['Test_KPI'] * n_points,
        'VALUE': values
    })
    
    return test_data

def test_matplotlib_utils():
    """æ¸¬è©¦ matplotlib_utils æ¨¡çµ„"""
    print("ğŸ“Š æ¸¬è©¦ matplotlib_utils æ¨¡çµ„...")
    
    try:
        from matplotlib_utils import (
            create_anomaly_plot, create_zscore_analysis_plot, 
            create_iqr_analysis_plot, create_isolation_forest_plot,
            create_dbscan_plot, create_comparison_plot, render_matplotlib_figure
        )
        print("âœ… matplotlib_utils æ¨¡çµ„å°å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ matplotlib_utils æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
        return False
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    test_data = generate_test_data()
    dates = pd.to_datetime(test_data['REPORT_TIME'])
    values = test_data['VALUE'].values
    
    # ç”Ÿæˆä¸€äº›ç•°å¸¸é»ç”¨æ–¼æ¸¬è©¦
    z_scores = np.abs((values - np.mean(values)) / np.std(values))
    anomalies = np.where(z_scores > 2.0)[0]
    
    print(f"   - æ•¸æ“šé»æ•¸: {len(values)}")
    print(f"   - ç•°å¸¸é»æ•¸: {len(anomalies)}")
    
    # æ¸¬è©¦å„ç¨®åœ–è¡¨
    tests = [
        ("é€šç”¨ç•°å¸¸æª¢æ¸¬åœ–", lambda: create_anomaly_plot(
            dates, values, anomalies, "æ¸¬è©¦ç•°å¸¸æª¢æ¸¬", scores=z_scores
        )),
        ("Z-Score åˆ†æåœ–", lambda: create_zscore_analysis_plot(
            dates, values, z_scores, 2.0, anomalies, "æ¸¬è©¦ Z-Score"
        )),
        ("IQR åˆ†æåœ–", lambda: create_iqr_analysis_plot(
            dates, values, anomalies, 1.5, "æ¸¬è©¦ IQR"
        ))
    ]
    
    for test_name, test_func in tests:
        try:
            fig = test_func()
            plt.close(fig)  # é—œé–‰åœ–è¡¨ä»¥ç¯€çœè¨˜æ†¶é«”
            print(f"   âœ… {test_name} - æˆåŠŸ")
        except Exception as e:
            print(f"   âŒ {test_name} - å¤±æ•—: {e}")
            return False
    
    return True

def test_advanced_anomaly_detection():
    """æ¸¬è©¦é«˜ç´šç•°å¸¸æª¢æ¸¬æ¨¡çµ„"""
    print("ğŸ¤– æ¸¬è©¦ advanced_anomaly_detection æ¨¡çµ„...")
    
    try:
        from advanced_anomaly_detection import (
            isolation_forest_analysis_page, dbscan_analysis_page,
            ensemble_anomaly_detection_page, perform_isolation_forest_analysis,
            perform_dbscan_analysis, perform_ensemble_anomaly_detection
        )
        print("âœ… advanced_anomaly_detection æ¨¡çµ„å°å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ advanced_anomaly_detection æ¨¡çµ„å°å…¥å¤±æ•—: {e}")
        return False
    
    # æ¸¬è©¦æ ¸å¿ƒåŠŸèƒ½
    test_data = generate_test_data()
    
    try:
        # æ¸¬è©¦ Isolation Forest åˆ†æï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        from sklearn.ensemble import IsolationForest
        from sklearn.preprocessing import StandardScaler
        
        features = test_data[['VALUE']].copy()
        features['rolling_mean'] = features['VALUE'].rolling(7).mean().fillna(method='bfill')
        features['rolling_std'] = features['VALUE'].rolling(7).std().fillna(0)
        
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features.values)
        
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        predictions = iso_forest.fit_predict(features_scaled)
        
        print(f"   âœ… Isolation Forest æ¸¬è©¦æˆåŠŸ - æª¢æ¸¬åˆ° {sum(predictions == -1)} å€‹ç•°å¸¸é»")
        
    except Exception as e:
        print(f"   âŒ Isolation Forest æ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    try:
        # æ¸¬è©¦ DBSCAN åˆ†æï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        from sklearn.cluster import DBSCAN
        
        features = test_data[['VALUE']].copy()
        features['value_normalized'] = (features['VALUE'] - features['VALUE'].mean()) / features['VALUE'].std()
        
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        labels = dbscan.fit_predict(features.values)
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = sum(labels == -1)
        
        print(f"   âœ… DBSCAN æ¸¬è©¦æˆåŠŸ - {n_clusters} å€‹èšé¡, {n_noise} å€‹ç•°å¸¸é»")
        
    except Exception as e:
        print(f"   âŒ DBSCAN æ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    return True

def test_time_series_analysis():
    """æ¸¬è©¦æ™‚é–“åºåˆ—åˆ†ææ¨¡çµ„"""
    print("ğŸ“ˆ æ¸¬è©¦ time_series_analysis æ¨¡çµ„...")
    
    try:
        # ä¸ç›´æ¥å°å…¥é é¢å‡½æ•¸ï¼Œåªæ¸¬è©¦æ ¸å¿ƒé‚è¼¯
        from scipy import stats
        import pandas as pd
        
        test_data = generate_test_data()
        values = test_data['VALUE'].values
        
        # æ¸¬è©¦è¶¨å‹¢åˆ†æ
        x = np.arange(len(values))
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, values)
        print(f"   âœ… è¶¨å‹¢åˆ†ææ¸¬è©¦æˆåŠŸ - æ–œç‡: {slope:.4f}, RÂ²: {r_value**2:.3f}")
        
        # æ¸¬è©¦é€±æœŸæ€§åˆ†æ
        if len(values) >= 14:
            fft = np.fft.fft(values)
            freqs = np.fft.fftfreq(len(values))
            power = np.abs(fft) ** 2
            print(f"   âœ… é€±æœŸæ€§åˆ†ææ¸¬è©¦æˆåŠŸ - FFT è¨ˆç®—å®Œæˆ")
        
        # æ¸¬è©¦è‡ªç›¸é—œåˆ†æ
        if len(values) >= 20:
            autocorr = [np.corrcoef(values[:-i], values[i:])[0,1] for i in range(1, min(10, len(values)//4))]
            max_autocorr = max(autocorr)
            print(f"   âœ… è‡ªç›¸é—œåˆ†ææ¸¬è©¦æˆåŠŸ - æœ€å¤§è‡ªç›¸é—œ: {max_autocorr:.3f}")
        
    except Exception as e:
        print(f"   âŒ æ™‚é–“åºåˆ—åˆ†ææ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    return True

def test_batch_monitoring():
    """æ¸¬è©¦æ‰¹é‡ç›£æ§æ¨¡çµ„"""
    print("ğŸ“Š æ¸¬è©¦ batch_monitoring æ¨¡çµ„...")
    
    try:
        # æ¸¬è©¦æ‰¹é‡ç•°å¸¸æª¢æ¸¬é‚è¼¯
        test_data = generate_test_data()
        
        # å‰µå»ºå¤šå€‹ KPI çš„æ¸¬è©¦æ•¸æ“š
        kpis = ['KPI_A', 'KPI_B', 'KPI_C']
        full_data = []
        
        for kpi in kpis:
            kpi_data = test_data.copy()
            kpi_data['KPI'] = kpi
            # ç‚ºä¸åŒ KPI æ·»åŠ ä¸åŒçš„è®Šç•°
            kpi_data['VALUE'] += np.random.normal(0, 5, len(kpi_data))
            full_data.append(kpi_data)
        
        combined_data = pd.concat(full_data, ignore_index=True)
        
        # æ¸¬è©¦æ‰¹é‡æª¢æ¸¬
        results = {}
        for kpi in kpis:
            kpi_subset = combined_data[combined_data['KPI'] == kpi]
            values = kpi_subset['VALUE'].values
            
            # ç°¡å–®çš„ Z-Score æª¢æ¸¬
            mean_val = np.mean(values)
            std_val = np.std(values)
            z_scores = np.abs((values - mean_val) / std_val)
            outliers = np.where(z_scores > 2.0)[0]
            
            results[kpi] = {
                'anomaly_count': len(outliers),
                'anomaly_rate': len(outliers) / len(values) * 100,
                'outliers': outliers,
                'values': values,
                'dates': kpi_subset['REPORT_TIME'].values
            }
        
        total_anomalies = sum(r['anomaly_count'] for r in results.values())
        avg_anomaly_rate = np.mean([r['anomaly_rate'] for r in results.values()])
        
        print(f"   âœ… æ‰¹é‡ç›£æ§æ¸¬è©¦æˆåŠŸ")
        print(f"      - ç›£æ§ KPI æ•¸é‡: {len(kpis)}")
        print(f"      - ç¸½ç•°å¸¸é»æ•¸: {total_anomalies}")
        print(f"      - å¹³å‡ç•°å¸¸ç‡: {avg_anomaly_rate:.2f}%")
        
    except Exception as e:
        print(f"   âŒ æ‰¹é‡ç›£æ§æ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    return True

def test_app_functionality():
    """æ¸¬è©¦ä¸»æ‡‰ç”¨ç¨‹å¼åŠŸèƒ½"""
    print("ğŸ­ æ¸¬è©¦ä¸»æ‡‰ç”¨ç¨‹å¼åŠŸèƒ½...")
    
    try:
        # æ¸¬è©¦ç•°å¸¸æª¢æ¸¬æ ¸å¿ƒåŠŸèƒ½
        test_data = generate_test_data()
        values = test_data['VALUE'].values
        
        # Z-Score æª¢æ¸¬
        mean_val = np.mean(values)
        std_val = np.std(values)
        z_scores = np.abs((values - mean_val) / std_val)
        z_outliers = np.where(z_scores > 2.0)[0]
        
        # IQR æª¢æ¸¬
        Q1 = np.percentile(values, 25)
        Q3 = np.percentile(values, 75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        iqr_outliers = np.where((values < lower_bound) | (values > upper_bound))[0]
        
        print(f"   âœ… çµ±è¨ˆæª¢æ¸¬æ¸¬è©¦æˆåŠŸ")
        print(f"      - Z-Score ç•°å¸¸é»: {len(z_outliers)}")
        print(f"      - IQR ç•°å¸¸é»: {len(iqr_outliers)}")
        
        # æ¸¬è©¦ç§»å‹•å¹³å‡æª¢æ¸¬
        window_size = 30
        if len(values) >= window_size:
            moving_avg = pd.Series(values).rolling(window_size).mean()
            deviations = np.abs((values - moving_avg) / moving_avg) * 100
            deviations = np.nan_to_num(deviations, 0)
            ma_outliers = np.where(deviations > 15.0)[0]
            print(f"      - ç§»å‹•å¹³å‡ç•°å¸¸é»: {len(ma_outliers)}")
        
    except Exception as e:
        print(f"   âŒ ä¸»æ‡‰ç”¨ç¨‹å¼æ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    return True

def run_all_tests():
    """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
    print("ğŸ§ª é–‹å§‹åŸ·è¡Œ matplotlib è½‰æ›æ¸¬è©¦...\n")
    
    tests = [
        ("matplotlib_utils", test_matplotlib_utils),
        ("advanced_anomaly_detection", test_advanced_anomaly_detection),
        ("time_series_analysis", test_time_series_analysis),
        ("batch_monitoring", test_batch_monitoring),
        ("app_functionality", test_app_functionality)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            if result:
                passed += 1
            print()  # ç©ºè¡Œåˆ†éš”
        except Exception as e:
            print(f"âŒ æ¸¬è©¦ {test_name} ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}\n")
    
    # ç¸½çµ
    print("=" * 60)
    print("ğŸ“‹ æ¸¬è©¦ç¸½çµ")
    print("=" * 60)
    print(f"âœ… é€šéæ¸¬è©¦: {passed}/{total}")
    print(f"âŒ å¤±æ•—æ¸¬è©¦: {total-passed}/{total}")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼matplotlib è½‰æ›æˆåŠŸå®Œæˆã€‚")
        print("âœ¨ ç³»çµ±å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹ä½¿ç”¨æ–°çš„è¦–è¦ºåŒ–åŠŸèƒ½ã€‚")
    else:
        print(f"\nâš ï¸  {total-passed} å€‹æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç›¸é—œæ¨¡çµ„ã€‚")
    
    print("\nğŸ“ æ³¨æ„äº‹é …:")
    print("1. ç¢ºä¿å·²å®‰è£æ‰€æœ‰å¿…è¦çš„å¥—ä»¶ (pip install -r requirements.txt)")
    print("2. matplotlib ä¸­æ–‡å­—å‹è¨­å®šå·²åŒ…å«åœ¨å„æ¨¡çµ„ä¸­")
    print("3. æ‰€æœ‰åœ–è¡¨ç¾åœ¨ä½¿ç”¨ matplotlib è€Œé plotly")
    print("4. ç•°å¸¸æª¢æ¸¬åˆ†æç¾åœ¨æä¾›æ›´è©³ç´°çš„çµ±è¨ˆä¿¡æ¯")
    
    return passed == total

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)