import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
from typing import Dict, List, Tuple, Optional
from scipy import stats
from scipy.stats import normaltest, jarque_bera, kstest, anderson
import warnings
warnings.filterwarnings('ignore')

def descriptive_statistics_page():
    """æ•˜è¿°çµ±è¨ˆåˆ†æé é¢"""
    st.header("ğŸ“Š æ•˜è¿°çµ±è¨ˆåˆ†æ")
    
    if st.session_state.fab_data is None:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³æ•¸æ“šä¸¦é¸æ“‡ FAB")
        st.info("ğŸ’¡ è«‹å…ˆå¾å·¦å´é¸å–®é¸æ“‡ã€ŒKPI å¿«é€Ÿåˆ†æã€è¼‰å…¥æ•¸æ“š")
        return
    
    fab_data = st.session_state.fab_data
    selected_fab = st.session_state.selected_fab
    available_kpis = st.session_state.available_kpis
    
    # é¡¯ç¤ºç•¶å‰é¸æ“‡
    st.info(f"ğŸ­ ç•¶å‰ FAB: **{selected_fab}** | ğŸ“Š å¯ç”¨ KPI: **{len(available_kpis)}** å€‹")
    
    # è¨ˆç®—æ‰€æœ‰ KPI çš„çµ±è¨ˆç‰¹æ€§
    kpi_profiles = analyze_all_kpis(fab_data, available_kpis)
    
    # é¡¯ç¤ºç¸½è¦½
    display_kpi_overview(kpi_profiles, selected_fab)
    
    # é¡¯ç¤ºè©³ç´°åˆ†æ
    display_detailed_analysis(kpi_profiles, fab_data)
    
    # æ–¹æ³•è«–å»ºè­°
    display_methodology_recommendations(kpi_profiles)

def analyze_all_kpis(fab_data: pd.DataFrame, kpis: List[str]) -> Dict:
    """åˆ†ææ‰€æœ‰ KPI çš„çµ±è¨ˆç‰¹æ€§"""
    profiles = {}
    
    for kpi in kpis:
        kpi_data = fab_data[fab_data['KPI'] == kpi]['VALUE'].dropna()
        
        if len(kpi_data) < 10:
            continue
            
        profile = analyze_single_kpi(kpi_data, kpi)
        profiles[kpi] = profile
    
    return profiles

def analyze_single_kpi(data: pd.Series, kpi_name: str) -> Dict:
    """åˆ†æå–®å€‹ KPI çš„çµ±è¨ˆç‰¹æ€§"""
    profile = {
        'name': kpi_name,
        'data': data,
        'n_obs': len(data),
        'basic_stats': {},
        'distribution_tests': {},
        'data_characteristics': {},
        'tags': [],
        'recommended_methods': []
    }
    
    # åŸºç¤çµ±è¨ˆé‡
    profile['basic_stats'] = calculate_basic_statistics(data)
    
    # åˆ†å¸ƒæª¢é©—
    profile['distribution_tests'] = perform_distribution_tests(data)
    
    # æ•¸æ“šç‰¹æ€§åˆ†æ
    profile['data_characteristics'] = analyze_data_characteristics(data)
    
    # ç”Ÿæˆæ¨™ç±¤
    profile['tags'] = generate_kpi_tags(profile)
    
    # æ–¹æ³•è«–å»ºè­°
    profile['recommended_methods'] = recommend_analysis_methods(profile)
    
    return profile

def calculate_basic_statistics(data: pd.Series) -> Dict:
    """è¨ˆç®—åŸºç¤çµ±è¨ˆé‡"""
    return {
        'mean': data.mean(),
        'median': data.median(),
        'std': data.std(),
        'var': data.var(),
        'min': data.min(),
        'max': data.max(),
        'q1': data.quantile(0.25),
        'q3': data.quantile(0.75),
        'iqr': data.quantile(0.75) - data.quantile(0.25),
        'skewness': stats.skew(data),
        'kurtosis': stats.kurtosis(data),
        'cv': data.std() / data.mean() if data.mean() != 0 else np.inf,
        'range': data.max() - data.min(),
        'mad': np.median(np.abs(data - data.median())),  # ä¸­ä½æ•¸çµ•å°åå·®
        'percentiles': {
            '5th': data.quantile(0.05),
            '95th': data.quantile(0.95),
            '99th': data.quantile(0.99)
        }
    }

def perform_distribution_tests(data: pd.Series) -> Dict:
    """åŸ·è¡Œåˆ†å¸ƒæª¢é©—"""
    tests = {}
    
    # å¸¸æ…‹æ€§æª¢é©—
    try:
        # Shapiro-Wilk æª¢é©— (é©åˆå°æ¨£æœ¬)
        if len(data) <= 5000:
            shapiro_stat, shapiro_p = stats.shapiro(data)
            tests['shapiro'] = {'statistic': shapiro_stat, 'p_value': shapiro_p}
        
        # D'Agostino-Pearson æª¢é©—
        dp_stat, dp_p = normaltest(data)
        tests['dagostino_pearson'] = {'statistic': dp_stat, 'p_value': dp_p}
        
        # Jarque-Bera æª¢é©—
        jb_stat, jb_p = jarque_bera(data)
        tests['jarque_bera'] = {'statistic': jb_stat, 'p_value': jb_p}
        
        # Kolmogorov-Smirnov æª¢é©— (èˆ‡æ¨™æº–å¸¸æ…‹åˆ†å¸ƒæ¯”è¼ƒ)
        ks_stat, ks_p = kstest(stats.zscore(data), 'norm')
        tests['kolmogorov_smirnov'] = {'statistic': ks_stat, 'p_value': ks_p}
        
        # Anderson-Darling æª¢é©—
        ad_result = anderson(data, dist='norm')
        tests['anderson_darling'] = {
            'statistic': ad_result.statistic,
            'critical_values': ad_result.critical_values,
            'significance_levels': ad_result.significance_level
        }
        
    except Exception as e:
        tests['error'] = str(e)
    
    return tests

def analyze_data_characteristics(data: pd.Series) -> Dict:
    """åˆ†ææ•¸æ“šç‰¹æ€§"""
    characteristics = {}
    
    # æ•¸æ“šé¡å‹æ¨æ–·
    characteristics['inferred_type'] = infer_data_type(data)
    
    # é›¶å€¼åˆ†æ
    zero_count = (data == 0).sum()
    characteristics['zero_proportion'] = zero_count / len(data)
    characteristics['has_zeros'] = zero_count > 0
    
    # ç¨€ç–æ€§åˆ†æ
    characteristics['sparsity'] = calculate_sparsity(data)
    
    # ç•°å¸¸å€¼åˆ†æ
    characteristics['outliers'] = detect_outliers_iqr(data)
    
    # è¶¨å‹¢åˆ†æ
    characteristics['trend'] = analyze_trend(data)
    
    # å¹³ç©©æ€§åˆ†æ (ç°¡åŒ–ç‰ˆ)
    characteristics['stationarity'] = analyze_stationarity(data)
    
    # é€±æœŸæ€§åˆ†æ
    characteristics['periodicity'] = analyze_periodicity(data)
    
    # å€¼åŸŸåˆ†æ
    characteristics['range_analysis'] = analyze_value_range(data)
    
    return characteristics

def infer_data_type(data: pd.Series) -> str:
    """æ¨æ–·æ•¸æ“šé¡å‹"""
    unique_values = data.nunique()
    total_values = len(data)
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºæ•´æ•¸
    is_integer = all(data.dropna() == data.dropna().astype(int))
    
    # æª¢æŸ¥å€¼åŸŸ
    min_val, max_val = data.min(), data.max()
    
    if unique_values == 2 and set(data.unique()) <= {0, 1}:
        return 'binary'
    elif unique_values <= 10 and is_integer and min_val >= 0:
        return 'categorical_ordinal'
    elif is_integer and min_val >= 0 and max_val <= 100:
        return 'count_small'
    elif is_integer and min_val >= 0:
        return 'count_large'
    elif 0 <= min_val and max_val <= 1:
        return 'probability'
    elif 0 <= min_val and max_val <= 100:
        return 'percentage'
    elif min_val >= 0:
        return 'continuous_positive'
    else:
        return 'continuous_general'

def calculate_sparsity(data: pd.Series) -> Dict:
    """è¨ˆç®—ç¨€ç–æ€§æŒ‡æ¨™"""
    total_count = len(data)
    non_zero_count = (data != 0).sum()
    zero_count = total_count - non_zero_count
    
    # è¨ˆç®—ä¸åŒé¡å‹çš„ç¨€ç–æ€§
    sparsity_metrics = {
        'zero_ratio': zero_count / total_count,
        'density': non_zero_count / total_count,
        'is_sparse': zero_count / total_count > 0.7,
        'consecutive_zeros': find_longest_consecutive_zeros(data),
        'zero_runs': count_zero_runs(data)
    }
    
    return sparsity_metrics

def find_longest_consecutive_zeros(data: pd.Series) -> int:
    """æ‰¾å‡ºæœ€é•·é€£çºŒé›¶å€¼åºåˆ—"""
    is_zero = data == 0
    consecutive_counts = []
    current_count = 0
    
    for val in is_zero:
        if val:
            current_count += 1
        else:
            if current_count > 0:
                consecutive_counts.append(current_count)
            current_count = 0
    
    if current_count > 0:
        consecutive_counts.append(current_count)
    
    return max(consecutive_counts) if consecutive_counts else 0

def count_zero_runs(data: pd.Series) -> int:
    """è¨ˆç®—é›¶å€¼åºåˆ—çš„æ•¸é‡"""
    is_zero = data == 0
    runs = 0
    in_run = False
    
    for val in is_zero:
        if val and not in_run:
            runs += 1
            in_run = True
        elif not val:
            in_run = False
    
    return runs

def detect_outliers_iqr(data: pd.Series) -> Dict:
    """ä½¿ç”¨ IQR æ–¹æ³•æª¢æ¸¬ç•°å¸¸å€¼"""
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    
    return {
        'count': len(outliers),
        'proportion': len(outliers) / len(data),
        'lower_bound': lower_bound,
        'upper_bound': upper_bound,
        'extreme_low': (data < lower_bound).sum(),
        'extreme_high': (data > upper_bound).sum()
    }

def analyze_trend(data: pd.Series) -> Dict:
    """åˆ†æè¶¨å‹¢"""
    x = np.arange(len(data))
    
    try:
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, data)
        
        return {
            'slope': slope,
            'r_squared': r_value ** 2,
            'p_value': p_value,
            'is_significant': p_value < 0.05,
            'direction': 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'flat',
            'strength': abs(r_value)
        }
    except:
        return {'error': 'Unable to compute trend'}

def analyze_stationarity(data: pd.Series) -> Dict:
    """åˆ†æå¹³ç©©æ€§ (ç°¡åŒ–ç‰ˆ)"""
    # åˆ†æ®µæª¢æŸ¥å‡å€¼å’Œæ–¹å·®çš„ç©©å®šæ€§
    n = len(data)
    first_half = data[:n//2]
    second_half = data[n//2:]
    
    mean_diff = abs(second_half.mean() - first_half.mean()) / data.std()
    var_ratio = second_half.var() / first_half.var() if first_half.var() > 0 else 1
    
    return {
        'mean_shift': mean_diff,
        'variance_ratio': var_ratio,
        'likely_stationary': mean_diff < 0.5 and 0.5 < var_ratio < 2.0
    }

def analyze_periodicity(data: pd.Series) -> Dict:
    """åˆ†æé€±æœŸæ€§ (ç°¡åŒ–ç‰ˆ)"""
    if len(data) < 20:
        return {'insufficient_data': True}
    
    # ç°¡å–®çš„è‡ªç›¸é—œåˆ†æ
    max_lag = min(len(data) // 4, 50)
    autocorr = [data.autocorr(lag=i) for i in range(1, max_lag + 1)]
    
    # æ‰¾å‡ºé¡¯è‘—çš„è‡ªç›¸é—œ
    significant_lags = [i+1 for i, corr in enumerate(autocorr) if abs(corr) > 0.3]
    
    return {
        'max_autocorr': max(autocorr) if autocorr else 0,
        'significant_lags': significant_lags,
        'has_periodicity': len(significant_lags) > 0
    }

def analyze_value_range(data: pd.Series) -> Dict:
    """åˆ†æå€¼åŸŸç‰¹æ€§"""
    return {
        'is_bounded_below': data.min() >= 0,
        'is_bounded_above': data.max() <= 100,  # å‡è¨­ç™¾åˆ†æ¯”é¡å‹
        'is_percentage_like': 0 <= data.min() and data.max() <= 100,
        'is_probability_like': 0 <= data.min() and data.max() <= 1,
        'has_negative_values': (data < 0).any(),
        'value_concentration': calculate_value_concentration(data)
    }

def calculate_value_concentration(data: pd.Series) -> Dict:
    """è¨ˆç®—å€¼é›†ä¸­åº¦"""
    value_counts = data.value_counts()
    total_count = len(data)
    
    # è¨ˆç®—ä¸åŒé›†ä¸­åº¦æŒ‡æ¨™
    top_1_ratio = value_counts.iloc[0] / total_count if len(value_counts) > 0 else 0
    top_5_ratio = value_counts.head(5).sum() / total_count if len(value_counts) > 0 else 0
    
    return {
        'unique_ratio': len(value_counts) / total_count,
        'top_1_concentration': top_1_ratio,
        'top_5_concentration': top_5_ratio,
        'is_highly_concentrated': top_1_ratio > 0.5
    }

def generate_kpi_tags(profile: Dict) -> List[str]:
    """ç”Ÿæˆ KPI æ¨™ç±¤"""
    tags = []
    
    basic_stats = profile['basic_stats']
    characteristics = profile['data_characteristics']
    distribution_tests = profile['distribution_tests']
    
    # æ•¸æ“šé¡å‹æ¨™ç±¤
    data_type = characteristics['inferred_type']
    tags.append(f"TYPE_{data_type.upper()}")
    
    # å¸¸æ…‹æ€§æ¨™ç±¤
    is_normal = check_normality(distribution_tests)
    tags.append("NORMAL" if is_normal else "NON_NORMAL")
    
    # ç¨€ç–æ€§æ¨™ç±¤
    if characteristics['sparsity']['is_sparse']:
        tags.append("SPARSE")
        tags.append(f"ZERO_HEAVY_{int(characteristics['sparsity']['zero_ratio']*100)}%")
    
    # ååº¦æ¨™ç±¤
    skewness = basic_stats['skewness']
    if abs(skewness) < 0.5:
        tags.append("SYMMETRIC")
    elif skewness > 0.5:
        tags.append("RIGHT_SKEWED")
    elif skewness < -0.5:
        tags.append("LEFT_SKEWED")
    
    # å³°åº¦æ¨™ç±¤
    kurtosis = basic_stats['kurtosis']
    if kurtosis > 1:
        tags.append("HEAVY_TAILED")
    elif kurtosis < -1:
        tags.append("LIGHT_TAILED")
    
    # è®Šç•°æ€§æ¨™ç±¤
    cv = basic_stats['cv']
    if cv < 0.1:
        tags.append("LOW_VARIABILITY")
    elif cv > 0.5:
        tags.append("HIGH_VARIABILITY")
    
    # ç•°å¸¸å€¼æ¨™ç±¤
    outlier_prop = characteristics['outliers']['proportion']
    if outlier_prop > 0.05:
        tags.append("OUTLIER_PRONE")
    
    # è¶¨å‹¢æ¨™ç±¤
    trend = characteristics['trend']
    if trend.get('is_significant', False):
        tags.append(f"TREND_{trend['direction'].upper()}")
    
    # å¹³ç©©æ€§æ¨™ç±¤
    if characteristics['stationarity']['likely_stationary']:
        tags.append("STATIONARY")
    else:
        tags.append("NON_STATIONARY")
    
    # é€±æœŸæ€§æ¨™ç±¤
    if characteristics['periodicity'].get('has_periodicity', False):
        tags.append("PERIODIC")
    
    return tags

def check_normality(distribution_tests: Dict) -> bool:
    """æª¢æŸ¥æ˜¯å¦ç¬¦åˆå¸¸æ…‹åˆ†å¸ƒ"""
    normal_tests = 0
    total_tests = 0
    
    for test_name, result in distribution_tests.items():
        if test_name in ['shapiro', 'dagostino_pearson', 'jarque_bera', 'kolmogorov_smirnov']:
            total_tests += 1
            if result.get('p_value', 0) > 0.05:  # ä¸æ‹’çµ•å¸¸æ…‹åˆ†å¸ƒå‡è¨­
                normal_tests += 1
    
    return normal_tests / total_tests > 0.5 if total_tests > 0 else False

def recommend_analysis_methods(profile: Dict) -> List[str]:
    """æ¨è–¦åˆ†ææ–¹æ³•"""
    recommendations = []
    tags = profile['tags']
    characteristics = profile['data_characteristics']
    
    # åŸºæ–¼æ•¸æ“šé¡å‹æ¨è–¦
    if 'TYPE_BINARY' in tags:
        recommendations.extend(['é‚è¼¯å›æ­¸', 'å¡æ–¹æª¢é©—', 'äºŒé …åˆ†å¸ƒåˆ†æ'])
    elif 'TYPE_COUNT_SMALL' in tags or 'TYPE_COUNT_LARGE' in tags:
        recommendations.extend(['æ³Šæ¾å›æ­¸', 'è² äºŒé …å›æ­¸', 'è¨ˆæ•¸æ¨¡å‹'])
    elif 'SPARSE' in tags:
        recommendations.extend(['ç¨€ç–äº‹ä»¶åˆ†æ', 'é›¶è†¨è„¹æ¨¡å‹', 'ç”Ÿå­˜åˆ†æ'])
    
    # åŸºæ–¼åˆ†å¸ƒç‰¹æ€§æ¨è–¦
    if 'NORMAL' in tags:
        recommendations.extend(['Z-Scoreæª¢æ¸¬', 'åŸºæ–¼å¸¸æ…‹åˆ†å¸ƒçš„æ§åˆ¶åœ–', 'Tæª¢é©—'])
    else:
        recommendations.extend(['éåƒæ•¸æª¢é©—', 'IQRç•°å¸¸æª¢æ¸¬', 'ä¸­ä½æ•¸åŸºç¤åˆ†æ'])
    
    # åŸºæ–¼æ™‚åºç‰¹æ€§æ¨è–¦
    if 'TREND_INCREASING' in tags or 'TREND_DECREASING' in tags:
        recommendations.extend(['è¶¨å‹¢åˆ†æ', 'å›æ­¸åˆ†æ', 'é æ¸¬æ¨¡å‹'])
    
    if 'PERIODIC' in tags:
        recommendations.extend(['å­£ç¯€æ€§åˆ†è§£', 'ARIMAæ¨¡å‹', 'FFTåˆ†æ'])
    
    if 'NON_STATIONARY' in tags:
        recommendations.extend(['å·®åˆ†åˆ†æ', 'ARIMAæ¨¡å‹', 'çµæ§‹è®Šé»æª¢æ¸¬'])
    
    # åŸºæ–¼è®Šç•°æ€§æ¨è–¦
    if 'HIGH_VARIABILITY' in tags:
        recommendations.extend(['ç©©å¥çµ±è¨ˆæ–¹æ³•', 'Modified Z-Score', 'ç™¾åˆ†ä½æ•¸æ–¹æ³•'])
    
    if 'OUTLIER_PRONE' in tags:
        recommendations.extend(['ç•°å¸¸å€¼æª¢æ¸¬', 'ç©©å¥å›æ­¸', 'æˆªå°¾çµ±è¨ˆ'])
    
    # ç§»é™¤é‡è¤‡ä¸¦é™åˆ¶æ•¸é‡
    recommendations = list(set(recommendations))[:8]
    
    return recommendations

def display_kpi_overview(profiles: Dict, fab_name: str):
    """é¡¯ç¤º KPI ç¸½è¦½"""
    st.subheader(f"ğŸ“ˆ {fab_name} KPI ç¸½è¦½")
    
    if not profiles:
        st.warning("ç„¡è¶³å¤ æ•¸æ“šé€²è¡Œåˆ†æ")
        return
    
    # çµ±è¨ˆæ‘˜è¦
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("KPI ç¸½æ•¸", len(profiles))
    
    with col2:
        normal_count = sum(1 for p in profiles.values() if 'NORMAL' in p['tags'])
        st.metric("å¸¸æ…‹åˆ†å¸ƒ KPI", normal_count)
    
    with col3:
        sparse_count = sum(1 for p in profiles.values() if 'SPARSE' in p['tags'])
        st.metric("ç¨€ç– KPI", sparse_count)
    
    with col4:
        trend_count = sum(1 for p in profiles.values() 
                         if any('TREND_' in tag for tag in p['tags']))
        st.metric("æœ‰è¶¨å‹¢ KPI", trend_count)
    
    # KPI åˆ†é¡åœ–è¡¨
    display_kpi_classification_charts(profiles)

def display_kpi_classification_charts(profiles: Dict):
    """é¡¯ç¤º KPI åˆ†é¡åœ–è¡¨"""
    # æ•¸æ“šé¡å‹åˆ†å¸ƒ
    type_counts = {}
    for profile in profiles.values():
        data_type = profile['data_characteristics']['inferred_type']
        type_counts[data_type] = type_counts.get(data_type, 0) + 1
    
    # å¸¸æ…‹æ€§åˆ†å¸ƒ
    normality_counts = {'å¸¸æ…‹': 0, 'éå¸¸æ…‹': 0}
    for profile in profiles.values():
        if 'NORMAL' in profile['tags']:
            normality_counts['å¸¸æ…‹'] += 1
        else:
            normality_counts['éå¸¸æ…‹'] += 1
    
    # å‰µå»ºåœ–è¡¨
    fig = make_subplots(
        rows=1, cols=3,
        subplot_titles=('æ•¸æ“šé¡å‹åˆ†å¸ƒ', 'å¸¸æ…‹æ€§åˆ†å¸ƒ', 'ç¨€ç–æ€§åˆ†å¸ƒ'),
        specs=[[{"type": "pie"}, {"type": "pie"}, {"type": "bar"}]]
    )
    
    # æ•¸æ“šé¡å‹é¤…åœ–
    fig.add_trace(
        go.Pie(labels=list(type_counts.keys()), values=list(type_counts.values()), name="æ•¸æ“šé¡å‹"),
        row=1, col=1
    )
    
    # å¸¸æ…‹æ€§é¤…åœ–
    fig.add_trace(
        go.Pie(labels=list(normality_counts.keys()), values=list(normality_counts.values()), name="å¸¸æ…‹æ€§"),
        row=1, col=2
    )
    
    # ç¨€ç–æ€§æ¢å½¢åœ–
    sparse_data = []
    for kpi_name, profile in profiles.items():
        zero_ratio = profile['data_characteristics']['sparsity']['zero_ratio']
        sparse_data.append({'KPI': kpi_name, 'Zero_Ratio': zero_ratio})
    
    sparse_df = pd.DataFrame(sparse_data)
    
    fig.add_trace(
        go.Bar(x=sparse_df['KPI'], y=sparse_df['Zero_Ratio'], name="é›¶å€¼æ¯”ä¾‹"),
        row=1, col=3
    )
    
    fig.update_layout(height=400, showlegend=False)
    st.plotly_chart(fig, use_container_width=True)

def display_detailed_analysis(profiles: Dict, fab_data: pd.DataFrame):
    """é¡¯ç¤ºè©³ç´°åˆ†æ"""
    st.subheader("ğŸ” è©³ç´° KPI åˆ†æ")
    
    # KPI é¸æ“‡
    selected_kpi = st.selectbox(
        "é¸æ“‡è¦è©³ç´°åˆ†æçš„ KPI:",
        options=list(profiles.keys()),
        key="detailed_analysis_kpi"
    )
    
    if selected_kpi not in profiles:
        return
    
    profile = profiles[selected_kpi]
    
    # é¡¯ç¤ºåŸºæœ¬ä¿¡æ¯
    display_kpi_basic_info(profile)
    
    # é¡¯ç¤ºçµ±è¨ˆæ¸¬è©¦çµæœ
    display_statistical_tests(profile)
    
    # é¡¯ç¤ºè¦–è¦ºåŒ–
    display_kpi_visualization(profile, fab_data, selected_kpi)

def display_kpi_basic_info(profile: Dict):
    """é¡¯ç¤º KPI åŸºæœ¬ä¿¡æ¯"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**ğŸ“Š åŸºç¤çµ±è¨ˆé‡**")
        stats_df = pd.DataFrame([
            ['è§€æ¸¬æ•¸', f"{profile['n_obs']:,}"],
            ['å¹³å‡å€¼', f"{profile['basic_stats']['mean']:.4f}"],
            ['ä¸­ä½æ•¸', f"{profile['basic_stats']['median']:.4f}"],
            ['æ¨™æº–å·®', f"{profile['basic_stats']['std']:.4f}"],
            ['è®Šç•°ä¿‚æ•¸', f"{profile['basic_stats']['cv']:.4f}"],
            ['ååº¦', f"{profile['basic_stats']['skewness']:.4f}"],
            ['å³°åº¦', f"{profile['basic_stats']['kurtosis']:.4f}"]
        ], columns=['çµ±è¨ˆé‡', 'å€¼'])
        st.dataframe(stats_df, use_container_width=True, hide_index=True)
    
    with col2:
        st.write("**ğŸ·ï¸ KPI æ¨™ç±¤**")
        tags_text = ""
        for tag in profile['tags']:
            color = get_tag_color(tag)
            tags_text += f'<span style="background-color: {color}; padding: 2px 6px; margin: 2px; border-radius: 3px; font-size: 12px;">{tag}</span>'
        
        st.markdown(tags_text, unsafe_allow_html=True)
        
        st.write("**ğŸ’¡ å»ºè­°æ–¹æ³•**")
        for method in profile['recommended_methods']:
            st.write(f"â€¢ {method}")

def get_tag_color(tag: str) -> str:
    """å–å¾—æ¨™ç±¤é¡è‰²"""
    if 'TYPE_' in tag:
        return '#e3f2fd'
    elif tag in ['NORMAL', 'STATIONARY']:
        return '#e8f5e8'
    elif tag in ['NON_NORMAL', 'NON_STATIONARY']:
        return '#fff3e0'
    elif 'SPARSE' in tag or 'ZERO_' in tag:
        return '#fce4ec'
    elif 'TREND_' in tag:
        return '#f3e5f5'
    else:
        return '#f5f5f5'

def display_statistical_tests(profile: Dict):
    """é¡¯ç¤ºçµ±è¨ˆæª¢é©—çµæœ"""
    st.write("**ğŸ§ª çµ±è¨ˆæª¢é©—çµæœ**")
    
    tests = profile['distribution_tests']
    if 'error' in tests:
        st.error(f"çµ±è¨ˆæª¢é©—éŒ¯èª¤: {tests['error']}")
        return
    
    test_results = []
    for test_name, result in tests.items():
        if test_name == 'anderson_darling':
            continue  # Anderson-Darling çµæœè¼ƒè¤‡é›œï¼Œæš«æ™‚è·³é
        
        p_value = result.get('p_value', None)
        if p_value is not None:
            significance = "é¡¯è‘—" if p_value < 0.05 else "ä¸é¡¯è‘—"
            interpretation = get_test_interpretation(test_name, p_value < 0.05)
            
            test_results.append({
                'æª¢é©—æ–¹æ³•': format_test_name(test_name),
                'På€¼': f"{p_value:.6f}",
                'é¡¯è‘—æ€§': significance,
                'è§£é‡‹': interpretation
            })
    
    if test_results:
        st.dataframe(pd.DataFrame(test_results), use_container_width=True, hide_index=True)

def format_test_name(test_name: str) -> str:
    """æ ¼å¼åŒ–æª¢é©—åç¨±"""
    name_map = {
        'shapiro': 'Shapiro-Wilk',
        'dagostino_pearson': "D'Agostino-Pearson",
        'jarque_bera': 'Jarque-Bera',
        'kolmogorov_smirnov': 'Kolmogorov-Smirnov'
    }
    return name_map.get(test_name, test_name)

def get_test_interpretation(test_name: str, is_significant: bool) -> str:
    """å–å¾—æª¢é©—çµæœè§£é‡‹"""
    if test_name in ['shapiro', 'dagostino_pearson', 'jarque_bera', 'kolmogorov_smirnov']:
        if is_significant:
            return "æ‹’çµ•å¸¸æ…‹åˆ†å¸ƒå‡è¨­"
        else:
            return "ä¸æ‹’çµ•å¸¸æ…‹åˆ†å¸ƒå‡è¨­"
    return ""

def display_kpi_visualization(profile: Dict, fab_data: pd.DataFrame, kpi_name: str):
    """é¡¯ç¤º KPI è¦–è¦ºåŒ–"""
    st.write("**ğŸ“ˆ KPI è¦–è¦ºåŒ–åˆ†æ**")
    
    # å–å¾— KPI æ•¸æ“š
    kpi_data = fab_data[fab_data['KPI'] == kpi_name].copy()
    kpi_data = kpi_data.sort_values('REPORT_TIME')
    
    # å‰µå»ºå­åœ–
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('æ™‚åºåœ–', 'åˆ†å¸ƒç›´æ–¹åœ–', 'Q-Qåœ–', 'ç®±å‹åœ–'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # æ™‚åºåœ–
    fig.add_trace(
        go.Scatter(x=kpi_data['REPORT_TIME'], y=kpi_data['VALUE'], 
                  mode='lines+markers', name='åŸå§‹æ•¸æ“š', line=dict(width=1)),
        row=1, col=1
    )
    
    # åˆ†å¸ƒç›´æ–¹åœ–
    fig.add_trace(
        go.Histogram(x=profile['data'], nbinsx=30, name='åˆ†å¸ƒ'),
        row=1, col=2
    )
    
    # Q-Q åœ– (èˆ‡å¸¸æ…‹åˆ†å¸ƒæ¯”è¼ƒ)
    data_sorted = np.sort(profile['data'])
    theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, len(data_sorted)))
    
    fig.add_trace(
        go.Scatter(x=theoretical_quantiles, y=data_sorted, 
                  mode='markers', name='Q-Qé»'),
        row=2, col=1
    )
    
    # æ·»åŠ  Q-Q åƒè€ƒç·š
    min_val, max_val = min(theoretical_quantiles), max(theoretical_quantiles)
    fig.add_trace(
        go.Scatter(x=[min_val, max_val], y=[min_val, max_val], 
                  mode='lines', name='ç†è«–ç·š', line=dict(dash='dash')),
        row=2, col=1
    )
    
    # ç®±å‹åœ–
    fig.add_trace(
        go.Box(y=profile['data'], name='ç®±å‹åœ–'),
        row=2, col=2
    )
    
    fig.update_layout(height=600, showlegend=False)
    st.plotly_chart(fig, use_container_width=True)

def display_methodology_recommendations(profiles: Dict):
    """é¡¯ç¤ºæ–¹æ³•è«–å»ºè­°"""
    st.subheader("ğŸ’¡ æ–¹æ³•è«–å»ºè­°")
    
    # çµ±è¨ˆæ‰€æœ‰æ¨è–¦æ–¹æ³•
    method_counts = {}
    for profile in profiles.values():
        for method in profile['recommended_methods']:
            method_counts[method] = method_counts.get(method, 0) + 1
    
    # æŒ‰é©ç”¨çš„ KPI æ•¸é‡æ’åº
    sorted_methods = sorted(method_counts.items(), key=lambda x: x[1], reverse=True)
    
    st.write("**ğŸ¯ é‡å°ç•¶å‰ FAB çš„æ¨è–¦åˆ†ææ–¹æ³• (æŒ‰é©ç”¨æ€§æ’åº):**")
    
    for method, count in sorted_methods:
        percentage = (count / len(profiles)) * 100
        st.write(f"â€¢ **{method}** - é©ç”¨æ–¼ {count} å€‹ KPI ({percentage:.1f}%)")
        
        # é¡¯ç¤ºé©ç”¨çš„ KPI
        applicable_kpis = [kpi for kpi, profile in profiles.items() 
                          if method in profile['recommended_methods']]
        
        with st.expander(f"æŸ¥çœ‹é©ç”¨ {method} çš„ KPI"):
            for kpi in applicable_kpis:
                kpi_tags = profiles[kpi]['tags']
                st.write(f"  - **{kpi}**: {', '.join(kpi_tags[:3])}...")

    # ç‰¹æ®Šå»ºè­°
    st.write("**âš ï¸ ç‰¹æ®Šæ³¨æ„äº‹é …:**")
    
    sparse_kpis = [kpi for kpi, profile in profiles.items() if 'SPARSE' in profile['tags']]
    if sparse_kpis:
        st.warning(f"ç¨€ç–æ•¸æ“š KPI ({', '.join(sparse_kpis)}): å»ºè­°ä½¿ç”¨å°ˆé–€çš„ç¨€ç–æ•¸æ“šåˆ†ææ–¹æ³•")
    
    binary_kpis = [kpi for kpi, profile in profiles.items() if 'TYPE_BINARY' in profile['tags']]
    if binary_kpis:
        st.info(f"äºŒå…ƒæ•¸æ“š KPI ({', '.join(binary_kpis)}): é¿å…ä½¿ç”¨é€£çºŒæ•¸æ“šåˆ†ææ–¹æ³•")
    
    non_normal_kpis = [kpi for kpi, profile in profiles.items() if 'NON_NORMAL' in profile['tags']]
    if len(non_normal_kpis) > len(profiles) * 0.7:
        st.warning("å¤§éƒ¨åˆ† KPI ä¸ç¬¦åˆå¸¸æ…‹åˆ†å¸ƒï¼Œå»ºè­°å„ªå…ˆä½¿ç”¨éåƒæ•¸æ–¹æ³•")